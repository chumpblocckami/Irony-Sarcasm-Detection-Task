{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot, make_dot_from_trace\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from torch import nn\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "# from sklearn.metrics import classification_report\n",
    "# from Attention_Augmented_Conv2d.attention_augmented_conv import AugmentedConv\n",
    "# # use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "# from ark_tweet_pos import CMUTweetTagger\n",
    "# import shlex\n",
    "# run_tagger_cmd = \"java -XX:ParallelGCThreads=10 -Xmx500m -jar ark_tweet_pos/ark-tweet-nlp-0.3.2.jar\"\n",
    "# import FeaturesText\n",
    "import wandb\n",
    "wandb.login()\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" Applies attention mechanism on the `context` using the `query`.\n",
    "\n",
    "    **Thank you** to IBM for their initial implementation of :class:`Attention`. Here is\n",
    "    their `License\n",
    "    <https://github.com/IBM/pytorch-seq2seq/blob/master/LICENSE>`__.\n",
    "\n",
    "    Args:\n",
    "        dimensions (int): Dimensionality of the query and context.\n",
    "        attention_type (str, optional): How to compute the attention score:\n",
    "\n",
    "            * dot: :math:`score(H_j,q) = H_j^T q`\n",
    "            * general: :math:`score(H_j, q) = H_j^T W_a q`\n",
    "\n",
    "    Example:\n",
    "\n",
    "         >>> attention = Attention(256)\n",
    "         >>> query = torch.randn(5, 1, 256)\n",
    "         >>> context = torch.randn(5, 5, 256)\n",
    "         >>> output, weights = attention(query, context)\n",
    "         >>> output.size()\n",
    "         torch.Size([5, 1, 256])\n",
    "         >>> weights.size()\n",
    "         torch.Size([5, 1, 5])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions, attention_type='general'):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        if attention_type not in ['dot', 'general']:\n",
    "            raise ValueError('Invalid attention type selected.')\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        if self.attention_type == 'general':\n",
    "            self.linear_in = nn.Linear(dimensions, dimensions, bias=False)\n",
    "\n",
    "        self.linear_out = nn.Linear(dimensions * 2, dimensions, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, query, context):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (:class:`torch.FloatTensor` [batch size, output length, dimensions]): Sequence of\n",
    "                queries to query the context.\n",
    "            context (:class:`torch.FloatTensor` [batch size, query length, dimensions]): Data\n",
    "                overwhich to apply the attention mechanism.\n",
    "\n",
    "        Returns:\n",
    "            :class:`tuple` with `output` and `weights`:\n",
    "            * **output** (:class:`torch.LongTensor` [batch size, output length, dimensions]):\n",
    "              Tensor containing the attended features.\n",
    "            * **weights** (:class:`torch.FloatTensor` [batch size, output length, query length]):\n",
    "              Tensor containing attention weights.\n",
    "        \"\"\"\n",
    "        batch_size, output_len, dimensions = query.size()\n",
    "        query_len = context.size(1)\n",
    "\n",
    "        if self.attention_type == \"general\":\n",
    "            query = query.reshape(batch_size * output_len, dimensions)\n",
    "            query = self.linear_in(query)\n",
    "            query = query.reshape(batch_size, output_len, dimensions)\n",
    "\n",
    "        # TODO: Include mask on PADDING_INDEX?\n",
    "\n",
    "        # (batch_size, output_len, dimensions) * (batch_size, query_len, dimensions) ->\n",
    "        # (batch_size, output_len, query_len)\n",
    "        attention_scores = torch.bmm(query, context.transpose(1, 2).contiguous())\n",
    "\n",
    "        # Compute weights across every context sequence\n",
    "        attention_scores = attention_scores.view(batch_size * output_len, query_len)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        attention_weights = attention_weights.view(batch_size, output_len, query_len)\n",
    "\n",
    "        # (batch_size, output_len, query_len) * (batch_size, query_len, dimensions) ->\n",
    "        # (batch_size, output_len, dimensions)\n",
    "        mix = torch.bmm(attention_weights, context)\n",
    "\n",
    "        # concat -> (batch_size * output_len, 2*dimensions)\n",
    "        combined = torch.cat((mix, query), dim=2)\n",
    "        combined = combined.view(batch_size * output_len, 2 * dimensions)\n",
    "\n",
    "        # Apply linear_out on every 2nd dimension of concat\n",
    "        # output -> (batch_size, output_len, dimensions)\n",
    "        output = self.linear_out(combined).view(batch_size, output_len, dimensions)\n",
    "        output = self.dropout(self.tanh(output))\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(baseline, self).__init__()\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(4, 3, kernel_size=1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1d.weight, gain=np.sqrt(2))\n",
    "        self.attention = Attention(768)\n",
    "        self.conv1d2 = nn.Conv1d(3, 2, kernel_size=1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1d2.weight, gain=np.sqrt(2))\n",
    "        self.conv1d3 = nn.Conv1d(2, 1, kernel_size=1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1d3.weight, gain=np.sqrt(2))\n",
    "        self.bgru = nn.GRU(input_size=768, hidden_size=384, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        self.drop3 = nn.Dropout(0.4)\n",
    "        self.dense1 = nn.Linear(768, 512)\n",
    "        torch.nn.init.xavier_uniform_(self.dense1.weight, gain=np.sqrt(2))\n",
    "        self.dense2 = nn.Linear(512,256)\n",
    "        torch.nn.init.xavier_uniform_(self.dense2.weight, gain=np.sqrt(2))\n",
    "        self.dense3 = nn.Linear(256,128)\n",
    "        torch.nn.init.xavier_uniform_(self.dense3.weight, gain=np.sqrt(2))\n",
    "        self.dense4 = nn.Linear(128,64)\n",
    "        torch.nn.init.xavier_uniform_(self.dense4.weight, gain=np.sqrt(2))\n",
    "        self.dense5 = nn.Linear(64,2)\n",
    "        \n",
    "    def forward(self, input1):\n",
    "        input_sentence = F.relu(self.conv1d(input1))\n",
    "        input_sentence = self.drop(input_sentence)\n",
    "        attention_1, _ = self.attention(input_sentence, input1)\n",
    "        \n",
    "        input_sentence = F.relu(self.conv1d2(attention_1))\n",
    "        input_sentence = self.drop(input_sentence)\n",
    "        attention_2, _ = self.attention(input_sentence, attention_1)\n",
    "        \n",
    "        input_sentence = F.relu(self.conv1d3(attention_2))\n",
    "        input_sentence = self.drop(input_sentence)\n",
    "        attention_3, _ = self.attention(input_sentence, attention_2) # N x 1 x 768\n",
    "        \n",
    "        gru, _ = self.bgru(attention_3)\n",
    "        gru = self.drop2(gru)\n",
    "        flattening = torch.squeeze(gru, 1)\n",
    "        \n",
    "        dense = F.relu(self.dense1(flattening))\n",
    "        dense = self.drop2(dense)\n",
    "        dense = F.relu(self.dense2(dense))\n",
    "        dense = self.drop3(dense)\n",
    "        dense = self.dense3(dense)\n",
    "        dense = self.drop2(dense)\n",
    "        dense = F.relu(self.dense4(dense))\n",
    "        dense = self.drop2(dense)\n",
    "        output = self.dense5(dense)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "tb = SummaryWriter('prova')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((16,4,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = SummaryWriter()\n",
    "tb.add_graph(model, x)\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
