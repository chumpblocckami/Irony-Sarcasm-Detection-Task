{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "# from tensorflow.keras import backend as K\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import  DistilBertConfig, TFDistilBertModel\n",
    "from transformers import RobertaConfig, TFRobertaModel\n",
    "import tensorflow as tf \n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "from textattack.augmentation import WordNetAugmenter\n",
    "from textattack.augmentation import EmbeddingAugmenter\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from tensorflow.keras.callbacks import History\n",
    "from keras_one_cycle_clr import keras_one_cycle_clr as ktool\n",
    "import CLR\n",
    "from tensorflow.keras.callbacks import History\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath('.')) \n",
    "data_dir = ROOT_DIR + '\\\\{}\\\\{}\\\\{}\\\\'.format('data', 'Embeddings_output', 'Irony_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/final_training_irony.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data[\"text\"].str.replace(r'@([^\\s:]+)', '')\n",
    "data['text'] = data['text'].str.replace(r'#([^\\s:]+)', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#pulizia url classico\n",
    "def remove_link(x):\n",
    "    text = re.sub(r'^https?:\\/\\/.[\\r\\n]', '', str(x), flags=re.MULTILINE)\n",
    "    return text\n",
    "#rimozione definitiva di tutti i link \n",
    "\n",
    "def remove_link2(x):\n",
    "    text = re.sub(r'http\\S+', '', x)\n",
    "    return text\n",
    "\n",
    "def remove_rest(x):\n",
    "    text = re.sub(r':\\S+', \"\", x)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(remove_link)\n",
    "data['text'] = data['text'].apply(remove_link2)\n",
    "data['text'] = data['text'].apply(remove_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data.text != ' ') & (data.text != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertConfig\n",
    "from transformers import RobertaTokenizer, RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = 'distilbert-base-uncased'\n",
    "# Defining RoBERTa tokinizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(roberta, do_lower_case=True, add_special_tokens=True,\n",
    "                                                max_length=20, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(roberta, do_lower_case=True, add_special_tokens=True,\n",
    "                                                max_length=20, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences, tokenizer):\n",
    "    input_ids, input_masks, input_segments = [],[],[]\n",
    "    for sentence in tqdm(sentences):\n",
    "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=20, pad_to_max_length=True, \n",
    "                                             return_attention_mask=True, return_token_type_ids=True,truncation=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])        \n",
    "        \n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_id, input_mask, input_seg = tokenize(train_txt.text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "distil_bert = 'distilbert-base-uncased'\n",
    "roberta = 'roberta-base'\n",
    "config = RobertaConfig(dropout=0.4, attention_dropout=0.4)\n",
    "config.output_hidden_states = True\n",
    "#transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)\n",
    "config = RobertaConfig.from_pretrained(roberta, output_hidden_states=True)\n",
    "config.output_hidden_states = True\n",
    "transformer_model = TFRobertaModel.from_pretrained(roberta, config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding extraction from transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# input_ids_in = tf.keras.layers.Input(shape=(512,), name='input_token', dtype='int32')\n",
    "# input_masks_in = tf.keras.layers.Input(shape=(512,), name='masked_token', dtype='int32') \n",
    "# #input_seg_in = tf.keras.layers.Input(shape = (1130,), name = 'segment_id', dtype = 'int32')\n",
    "# embedding_layer = transformer_model([input_ids_in, input_masks_in])[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_ids_in = tf.keras.layers.Input(shape=(20,), name='input_token', dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(20,), name='masked_token', dtype='int32') \n",
    "\n",
    "embedding_layer = transformer_model([input_ids_in, input_masks_in])[0] \n",
    "\n",
    "model2 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator for extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_batch_generator(input_id, input_mask, y, batch_size=1):\n",
    "    while True:\n",
    "        n_batches_per_epoch = input_id.shape[0]//batch_size\n",
    "        for i in range(n_batches_per_epoch):\n",
    "            index_batch = range(input_id.shape[0])[batch_size*i:batch_size*(i+1)]       \n",
    "            x_batch_id = input_id[index_batch,:]\n",
    "            x_batch_mask = input_mask[index_batch,:]\n",
    "            y_batch = y[index_batch,:]\n",
    "            yield [x_batch_id, x_batch_mask], np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(\n",
    "    np.array(train_txt.label), num_classes=2, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction one by one document and storage each array directly to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id sequenziali \n",
    "for i in tqdm(range(input_id.shape[0])):\n",
    "    output = model2.predict([input_id[i,:].reshape(1,-1), input_mask[i, :].reshape(1,-1)], verbose = 0)\n",
    "    \n",
    "    np.save('../data/Embeddings_output/Irony_Data/doc_{}.npy'.format(i), output)\n",
    "    np.save('../data/Embeddings_output/Irony_Data/target_{}.npy'.format(i), y_train[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator flow from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch_generator(batch_size = 64):\n",
    "    \n",
    "#     while True:    \n",
    "    folder = os.listdir(data_dir)\n",
    "    n_batches_per_epoch = (y_train.shape[0] - 4000)//batch_size\n",
    "    for i in range(n_batches_per_epoch):\n",
    "\n",
    "        index_batch = range(4000, len(y_train))[batch_size*i:batch_size*(i+1)]\n",
    "        batch = [np.load('{}\\\\{}'.format(data_dir, 'doc_{}.npy'.format(i))) for i in index_batch]\n",
    "\n",
    "        x_batch = np.squeeze(np.asarray(batch))\n",
    "        y_batch = [np.load('{}\\\\{}'.format(data_dir, 'target_{}.npy'.format(i))) for i in index_batch]\n",
    "\n",
    "        yield x_batch, np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_batch_generator(batch_size = 64):\n",
    "    \n",
    "#     while True:\n",
    "    folder = os.listdir(data_dir[:73] + '\\\\Irony_validation\\\\')\n",
    "\n",
    "    n_batches_per_epoch = 4000//batch_size\n",
    "\n",
    "    for i in range(n_batches_per_epoch):\n",
    "\n",
    "        index_batch = range(4000)[batch_size*i:batch_size*(i+1)]\n",
    "        batch = [np.load('{}\\\\{}'.format(data_dir[:73] + '\\\\Irony_validation', 'doc_{}.npy'.format(i))) for i in index_batch]\n",
    "\n",
    "        x_batch = np.squeeze(np.asarray(batch))\n",
    "        y_batch = [np.load('{}\\\\{}'.format(data_dir[:73] + '\\\\Irony_validation', 'target_{}.npy'.format(i))) for i in index_batch]\n",
    "\n",
    "        yield x_batch, np.array(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.mdl_last_transformer.hdf5', save_best_only=True, monitor='train_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input(shape=(20, 768), name='input_emb')\n",
    "#norm = tf.keras.layers.BatchNormalization()(input_layer)\n",
    "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(input_layer)\n",
    "X = tf.keras.layers.Concatenate(axis=-1)([X, input_layer])\n",
    "X = tf.keras.layers.MaxPooling1D(20)(X)\n",
    "X = tf.keras.layers.SpatialDropout1D(0.4)(X)\n",
    "X = tf.keras.layers.Flatten()(X)\n",
    "\n",
    "#X = tf.keras.layers.Dense(512, activation=\"relu\")(X)\n",
    "#X = tf.keras.layers.Dropout(0.4)(X)\n",
    "X = tf.keras.layers.Dense(128, activation=\"relu\")(X)\n",
    "X = tf.keras.layers.Dropout(0.25)(X)\n",
    "X = tf.keras.layers.Dense(2, activation='softmax')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate= 2e-5, epsilon= 1e-3)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer= opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr = CLR.CyclicLR(base_lr=0.00001, max_lr=0.01,\n",
    "                        step_size=(len(y_train) - 15000)//50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_batch_generator(50), epochs=25, steps_per_epoch=(len(y_train) - 4000)//50, verbose=1, callbacks=[history], validation_data= valid_batch_generator(50),validation_steps=4000//50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.grid(linestyle = 'dashed')\n",
    "plt.plot(history.history['acc'], c = 'green')\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Accuracy Metric with respect to RCNN-Roberta')\n",
    "plt.legend(['Train Set', 'Validation Set'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'D:\\Data_Science_all\\MSC_2_anno\\Tesi_Irony_Sarcasm\\thesis_latex\\img\\accuracy_trasformer_irony.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Loss\"\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.grid(linestyle = 'dashed')\n",
    "plt.plot(history.history['loss'], c = 'green')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Cross Entropy Loss with respect to RCNN-Roberta')\n",
    "plt.legend(['Train Set', 'Validation Set'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'D:\\Data_Science_all\\MSC_2_anno\\Tesi_Irony_Sarcasm\\thesis_latex\\img\\loss_trasformer_irony.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model_irony.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_irony.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction = model.predict(train_batch_generator(4), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prediction = model.predict(valid_batch_generator(5), verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sem = pd.read_csv('../data/SemEval2018-Task3/datasets/goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt', sep='\\t')\n",
    "sem.rename({'Tweet text': 'text', 'Label' : 'label'}, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepro(data):\n",
    "    \n",
    "    data['text'] = data[\"text\"].str.replace(r'@([^\\s:]+)', '')\n",
    "    data['text'] = data['text'].str.replace(r'#([^\\s:]+)', '')\n",
    "    data['text'] = data['text'].apply(remove_link)\n",
    "    data['text'] = data['text'].apply(remove_link2)\n",
    "    data['text'] = data['text'].apply(remove_rest)\n",
    "#     data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "    \n",
    "    input_id, input_mask, input_seg = tokenize(data.text, tokenizer)\n",
    "    \n",
    "    return input_id, input_mask, input_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_sem, input_mask_sem, input_seg_sem = text_prepro(sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(input_id_sem.shape[0])):\n",
    "    output = model2.predict([input_id_sem[i,:].reshape(1,-1), input_mask_sem[i, :].reshape(1,-1)], verbose = 0)\n",
    "    \n",
    "    np.save('../data/Embeddings_output/Irony_test/doc_{}.npy'.format(i), output)\n",
    "    np.save('../data/Embeddings_output/Irony_test/target_{}.npy'.format(i), sem.label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = ROOT_DIR + '\\\\{}\\\\{}\\\\{}\\\\'.format('data', 'Embeddings_output', 'Irony_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch_generator(batch_size = 784, n_batches = 64):\n",
    "        \n",
    "        data_dir2 = data_dir\n",
    "\n",
    "        n_batches_per_epoch = n_batches//batch_size\n",
    "\n",
    "        for i in range(n_batches_per_epoch):\n",
    "\n",
    "            index_batch = range(n_batches)[batch_size*i:batch_size*(i+1)]\n",
    "            batch = [np.load('{}\\\\{}'.format(data_dir2, 'doc_{}.npy'.format(i))) for i in index_batch]\n",
    "\n",
    "            x_batch = np.squeeze(np.asarray(batch))\n",
    "            y_batch = [np.load('{}\\\\{}'.format(data_dir2, 'target_{}.npy'.format(i))) for i in index_batch]\n",
    "\n",
    "            yield x_batch, np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.predict(test_batch_generator(2, len(sem.label)), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_prediction.argmax(axis = 1), sem.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lab(x):\n",
    "    new_lab_list = []\n",
    "    for i in x:\n",
    "        if i == 0:\n",
    "            new_lab = '1:1'\n",
    "        else:\n",
    "            new_lab =  '2:0'\n",
    "            \n",
    "        new_lab_list.append(new_lab)\n",
    "        \n",
    "    return new_lab_list\n",
    "\n",
    "def mark_error(actual, predicted):\n",
    "    mark_list = []\n",
    "    for i,j in zip(actual, predicted):\n",
    "        if i != j:\n",
    "            mark = '+'\n",
    "        else:\n",
    "            mark = np.nan\n",
    "            \n",
    "        mark_list.append(mark)\n",
    "        \n",
    "    return mark_list\n",
    "\n",
    "def get_proba_distrib(clf_proba):\n",
    "    proba_ast = []\n",
    "    for i,j in zip(clf_proba[:,0], clf_proba[:,1]):\n",
    "        if i > j:\n",
    "            proba = ['*{}'.format(str(i.round(5))), str(j.round(5))]\n",
    "        else:\n",
    "            proba = [str(i.round(5)), '*{}'.format(str(j.round(5)))]\n",
    "            \n",
    "        proba_ast.append(proba)\n",
    "        \n",
    "    return np.array(proba_ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outpupt_bma(pred,proba, ground_truth):\n",
    "    \n",
    "    \n",
    "    actual = normalize_lab(ground_truth)\n",
    "    predicted = normalize_lab(pred)\n",
    "    \n",
    "    error = mark_error(actual, predicted)\n",
    "    conta = 0\n",
    "    lista_ins = []\n",
    "    for i in range(len(ground_truth)):\n",
    "        conta += 1\n",
    "        if conta == int(len(ground_truth)/10) + 2:\n",
    "            conta = 1\n",
    "     \n",
    "        lista_ins.append(conta)  \n",
    "        \n",
    "    instanc = lista_ins\n",
    "    \n",
    "    predict_proba = proba\n",
    "    \n",
    "    distribution = get_proba_distrib(predict_proba)\n",
    "    \n",
    "    final_df = pd.DataFrame(instanc, columns=['inst#'])\n",
    "    \n",
    "    final_df['actual'] = actual\n",
    "    \n",
    "    final_df['predicted'] = predicted\n",
    "    \n",
    "    final_df['error'] = error\n",
    "\n",
    "    final_df['distribution'] = distribution[:, 0]\n",
    "    \n",
    "    final_df[''] = distribution[:,1]\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_outpupt_bma(test_prediction.argmax(axis = 1), test_prediction,  sem.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\Data_Science_all\\MSC_2_anno\\Tesi_Irony_Sarcasm\\Code\\BMA\\results_semeval\\input\\prediction_file\\transformer_model_based.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
