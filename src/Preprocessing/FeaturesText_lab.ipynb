{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import emot \n",
    "from string import punctuation\n",
    "import sys\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath('.'))\n",
    "sys.path.insert(0,'C:/Users/loren/Anaconda3/envs/tf_thesis/lib/site-packages/textblob')\n",
    "sys.path.insert(1,'C:/Users/loren/Anaconda3/envs/tf_thesis/lib/site-packages')\n",
    "sys.path.insert(2,'C:/Users/loren/Anaconda3/envs/tf_thesis/lib/site-packages/vaderSentiment')\n",
    "\n",
    "from textblob import TextBlob\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"are not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    global ROOT_DIR\n",
    "    with open('{}\\\\{}'.format(ROOT_DIR,path),encoding='utf8') as f:\n",
    "        data = f.readlines()\n",
    "        f.close()\n",
    "    return data\n",
    "\n",
    "def read_csv(path,sep):\n",
    "    global ROOT_DIR\n",
    "    data = pd.read_csv('{}\\\\{}'.format(ROOT_DIR,path), sep = sep)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing_text:\n",
    "    \"\"\"\n",
    "    txt_file: pandas df with the text column named 'text'\n",
    "    language: type of language to use\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, txt_file, language = 'en_core_web_sm',\n",
    "                remove_mentions = True, remove_hashtags = True, lowercase = True):\n",
    "        \n",
    "        self.txt_file = txt_file\n",
    "        self.remove_mentions = remove_mentions\n",
    "        self.remove_hashtags = remove_hashtags\n",
    "        self.lowercase = lowercase\n",
    "        \n",
    "        self.nlp = spacy.load(language)\n",
    "        self.stopwords = self.nlp.Defaults.stop_words\n",
    "        self.initialism_list = []\n",
    "        self.onomatopoeic_list = []\n",
    "        \n",
    "        for i in read_txt('Code/list_feature/initialism.list'):\n",
    "            self.initialism_list.append(re.search('([A-Z]*[a-z]*)', i)[0])\n",
    "    \n",
    "        for i in read_txt('Code/list_feature/onomatopoeic.list'):\n",
    "            self.onomatopoeic_list.append(re.search('([A-Z]*[a-z]*)', i)[0])\n",
    "        \n",
    "    def get_clean_df(self):\n",
    "        \n",
    "        df = self.preprocess_all(self.txt_file, self.remove_mentions, self.remove_hashtags, self.lowercase)\n",
    "        return df\n",
    "    \n",
    "    def preprocess_all(self, txt_file, remove_mentions = True, remove_hashtags = True, lowercase = True):\n",
    "\n",
    "        if remove_mentions:\n",
    "            txt_file['text'] = txt_file[\"text\"].str.replace(r'@([^\\s:]+)', '')\n",
    "\n",
    "        if remove_hashtags:\n",
    "            txt_file['text'] = txt_file['text'].str.replace(r'#([^\\s:]+)', '')\n",
    "\n",
    "        #Remove contractions and link (urls..)\n",
    "        txt_file['text'] = txt_file['text'].apply(self.remove_link)\n",
    "        txt_file['text'] = txt_file['text'].apply(self.remove_link2)\n",
    "        txt_file['no_contr_text'] = txt_file.text.apply(self.remove_contraction)\n",
    "\n",
    "        #Extract emoji infos\n",
    "        txt_file['Emoji'] = txt_file.no_contr_text.apply(self.extract_emoji)\n",
    "\n",
    "        #Remove useless spaces\n",
    "        txt_file[\"no_contr_text\"]  = txt_file[\"no_contr_text\"].replace('\\s+', ' ', regex=True)\n",
    "\n",
    "        #Extract expressions\n",
    "        txt_file['expressions_onomato'] = txt_file.no_contr_text.apply(self.extract_expressions, expression_list = self.onomatopoeic_list)\n",
    "        txt_file['expressions_initialism'] = txt_file.no_contr_text.apply(self.extract_expressions, expression_list = self.initialism_list)\n",
    "\n",
    "        #to lower case\n",
    "        if lowercase:\n",
    "            txt_file[\"no_contr_text\"] = txt_file.no_contr_text.str.lower()\n",
    "\n",
    "        #punctuaction\n",
    "        txt_file['punctuation'] = txt_file.no_contr_text.apply(self.count_punctuation)\n",
    "\n",
    "        #Remove all the stuff that aren't alphanumeric characters\n",
    "        txt_file['removed_nowords'] = txt_file.no_contr_text.str.replace(r'(\\W+)', ' ')\n",
    "\n",
    "        #Get polarity and subjectivity information\n",
    "        txt_file['polarity'] = txt_file.no_contr_text.apply(self.sentiment_info,type_sent = 0)\n",
    "        txt_file['subjectivity'] = txt_file.no_contr_text.apply(self.sentiment_info,type_sent = 1)\n",
    "\n",
    "        #Lemmatization, part of speech, name entity recognition\n",
    "        lem, pos, ner = self.preprocess_pipe(txt_file['removed_nowords'], self.nlp, )\n",
    "\n",
    "        txt_file['text_lemmatized'] = lem\n",
    "        txt_file['text_lemmatized'] = txt_file['text_lemmatized'].apply(self.remove_comma)\n",
    "        txt_file['pos'] = pos\n",
    "        txt_file['ner'] = ner\n",
    "\n",
    "        return txt_file            \n",
    "    \n",
    "    @staticmethod\n",
    "    def sentiment_info(text, type_sent = 0):\n",
    "        '''\n",
    "        type_sent: int 0/1, where 0 means get the polarity of the document, and 1 means get the subjectivity of the document\n",
    "        '''\n",
    "        text = text.encode('unicode-escape').decode('ASCII')\n",
    "\n",
    "        if type_sent == 0:\n",
    "            sentiment = TextBlob(text).sentiment[0]\n",
    "        else:\n",
    "            sentiment = TextBlob(text).sentiment[1]\n",
    "\n",
    "        return sentiment\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_expressions(x, expression_list):\n",
    "\n",
    "        expression_diz = {k : 0 for k in expression_list}\n",
    "        text = x.split(' ')\n",
    "\n",
    "        for i in text:\n",
    "            if i in expression_diz:\n",
    "                expression_diz[i] += 1\n",
    "\n",
    "        return expression_diz\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_emoji(text):\n",
    "        try:\n",
    "            emoticons_list = emot.emoticons(text)['value']\n",
    "\n",
    "        except TypeError:\n",
    "            emoticons_list = []\n",
    "\n",
    "        try:\n",
    "            emoji_list = emot.emoji(text)['value']\n",
    "\n",
    "        except TypeError:\n",
    "            emoji_list = []\n",
    "        emo_list = emoticons_list + emoji_list\n",
    "\n",
    "        return emo_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_punctuation(text):\n",
    "\n",
    "        counts = Counter(text)  # counts all occurences\n",
    "        punct_diz = {k : 0 for k in punctuation}\n",
    "\n",
    "        for i in counts:\n",
    "            if i in punct_diz:\n",
    "                punct_diz[i] = counts[i]\n",
    "\n",
    "        return punct_diz\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_comma(x):\n",
    "        filtered = [i for i in x if i.strip()]\n",
    "        return filtered    \n",
    "    \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_pipe(texts, nlp):\n",
    "        \n",
    "        def lemmatize_pipe(doc):\n",
    "            lemma_list = [str(tok.lemma_) for tok in doc if not tok.is_stop] \n",
    "            return lemma_list\n",
    "\n",
    "        def pos_pipe(doc):\n",
    "            pos_list = [tok.pos_ for tok in doc]\n",
    "            return pos_list\n",
    "\n",
    "        def ner_pipe(x):\n",
    "            ner_list = [token.label_ for token in x.ents]\n",
    "            return ner_list        \n",
    "        \n",
    "        preproc_pipe_lemma = []\n",
    "        preproc_pipe_pos = []\n",
    "        preproc_pipe_ner = []\n",
    "\n",
    "        for doc in nlp.pipe(texts, batch_size=20,  n_threads=12):\n",
    "            preproc_pipe_lemma.append(lemmatize_pipe(doc))\n",
    "            preproc_pipe_pos.append(pos_pipe(doc))\n",
    "            preproc_pipe_ner.append(ner_pipe(doc))\n",
    "\n",
    "        return preproc_pipe_lemma, preproc_pipe_pos, preproc_pipe_ner\n",
    "           \n",
    "    #clean url\n",
    "    @staticmethod\n",
    "    def remove_link(x):\n",
    "        text = re.sub(r'^https?:\\/\\/.[\\r\\n]', '', x, flags=re.MULTILINE)\n",
    "        return text\n",
    "\n",
    "    #removes other link \n",
    "    @staticmethod\n",
    "    def remove_link2(x):\n",
    "        text = re.sub(r'http\\S+', '', x)\n",
    "        return text    \n",
    "    @staticmethod\n",
    "    def remove_contraction(text):\n",
    "        for word in text.split():\n",
    "            if word.lower() in contractions:\n",
    "                text = text.replace(word, contractions[word.lower()])\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_emoji(x):\n",
    "        x = re.sub(r\"(\\<u+\\S*>)\", \"\", x)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def map_nlp(x):\n",
    "        x = nlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeatures:\n",
    "    \"\"\"\n",
    "    This class must be used after the preprocessing text phase \n",
    "    final_txt: pandas df extracted from the preprocessing class\n",
    "    mod_pos: 'pos_sequences' (good if you want to take into account temporal dependecies), otherwise its a frequency matrix\n",
    "    svd_transform: if all the features (except for polarity/subjectivity information) need to be transformed through svd\n",
    "    \"\"\"\n",
    "    def __init__(self, final_txt, mod_pos, svd_transform = True):\n",
    "        \n",
    "        self.final_txt = final_txt\n",
    "        self.mode_pos = mod_pos\n",
    "        self.svd_all = svd_transform\n",
    "        \n",
    "    #extract all the features for training\n",
    "    def get_all_features_train(self, ngram_range, dimensionality):\n",
    "        \n",
    "        punctuation_matrix = self.get_punctuaction()\n",
    "        initialism_matrix = self.get_expres_initialism()\n",
    "        onomato_matrix = self.get_expres_onomato()\n",
    "        polarity_subj_matrix = self.polarity_subjectivity_features()\n",
    "        emoji_matrix = self.CountVect('Emoji')\n",
    "        pos_matrix = self.select_pos_representation()\n",
    "        tfidfsvd_word_matrix = self.tfidf_ngrams_svd_text(ngram_range, dimensionality)\n",
    "        \n",
    "        if self.svd_all:\n",
    "            self.svdT2_punct = TruncatedSVD(n_components=10)\n",
    "            svdTFit_punctuation = self.svdT2_punct.fit_transform(punctuation_matrix)\n",
    "            \n",
    "            self.emoji_svdT3 = TruncatedSVD(n_components=50)\n",
    "            svdTFit_emoji = self.emoji_svdT3.fit_transform(emoji_matrix)\n",
    "            \n",
    "            self.svdT4_onomato = TruncatedSVD(n_components=10)\n",
    "            svdTFit_onomato = self.svdT4_onomato.fit_transform(onomato_matrix)\n",
    "            \n",
    "            self.svdT5_initialism = TruncatedSVD(n_components=10)\n",
    "            svdTFit_initialism = self.svdT5_initialism.fit_transform(onomato_matrix)\n",
    "            \n",
    "            return pos_matrix, tfidfsvd_word_matrix, svdTFit_punctuation, svdTFit_emoji, svdTFit_onomato, svdTFit_initialism, polarity_subj_matrix\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return pos_matrix, tfidfsvd_word_matrix, punctuation_matrix,emoji_matrix,onomato_matrix,initialism_matrix, polarity_subj_matrix\n",
    "    \n",
    "    #use this after applied training features extraction\n",
    "    def get_all_features_test(self):\n",
    "        \n",
    "        punctuation_matrix = self.get_punctuaction()\n",
    "        initialism_matrix = self.get_expres_initialism()\n",
    "        onomato_matrix = self.get_expres_onomato()\n",
    "        polarity_subj_matrix = self.polarity_subjectivity_features()\n",
    "        \n",
    "        counted_values = self.cv.transform(self.final_txt['Emoji']).toarray()\n",
    "        emoji_matrix = pd.DataFrame(counted_values, columns=self.cv.get_feature_names())\n",
    "        \n",
    "        if self.mode_pos == 'pos_sequences':\n",
    "            sequences_pos = self.pos_tokenizer.texts_to_sequences(self.final_txt['pos'].astype(str))\n",
    "            pos = pad_sequences(sequences_pos, maxlen=len(max(sequences_pos, key=len)), padding='post')\n",
    "        else: \n",
    "            try:\n",
    "                pos = self.pos_vectorizer.transform(self.final_txt.txt_lemmatized).todense()\n",
    "            except AttributeError:\n",
    "                new_pos = final_txt.pos.apply(lambda x: ' '.join(x))\n",
    "                pos = self.pos_vectorizer.transform(new_pos).todense()\n",
    "        try:\n",
    "            tfs = self.word_tfidf.transform(self.final_txt.text_lemmatized)\n",
    "        except AttributeError:\n",
    "            corpus = self.final_txt.text_lemmatized.apply(lambda x: ' '.join(x))\n",
    "            tfs = self.word_tfidf.transform(corpus)\n",
    "        tfs = tfs.astype('float32')\n",
    "        tfidfsvd_word_matrix = self.word_svdT.transform(tfs)\n",
    "        \n",
    "        if self.svd_all:\n",
    "            svdTFit_punctuation = self.svdT2_punct.transform(punctuation_matrix)\n",
    "            svdTFit_emoji = self.emoji_svdT3.transform(emoji_matrix)\n",
    "            svdTFit_onomato = self.svdT4_onomato.transform(onomato_matrix)\n",
    "            svdTFit_initialism = self.svdT5_initialism.fit_transform(onomato_matrix)\n",
    "            \n",
    "            return pos, tfidfsvd_word_matrix, svdTFit_punctuation, svdTFit_emoji, svdTFit_onomato, svdTFit_initialism, polarity_subj_matrix\n",
    "        else:\n",
    "            return pos, tfidfsvd_word_matrix, punctuation_matrix,emoji_matrix,onomato_matrix,initialism_matrix, polarity_subj_matrix\n",
    "    \n",
    "    #Choose the part of speech representation, pos_sequences is zero padded to the max lenght (sequence information)\n",
    "    #otherwise is a count frequency matrix\n",
    "    def select_pos_representation(self):\n",
    "        if self.mode_pos == 'pos_sequences':\n",
    "            self.pos_tokenizer = Tokenizer(num_words=20)\n",
    "            self.pos_tokenizer.fit_on_texts(self.final_txt['pos'].astype(str))\n",
    "            sequences_pos = self.pos_tokenizer.texts_to_sequences(self.final_txt['pos'].astype(str))\n",
    "            pos = pad_sequences(sequences_pos, maxlen=len(max(sequences_pos, key=len)), padding='post')\n",
    "        else:\n",
    "            self.pos_vectorizer = CountVectorizer()\n",
    "            try:\n",
    "                pos = self.pos_vectorizer.fit_transform(self.final_txt.txt_lemmatized).todense()\n",
    "            except AttributeError:\n",
    "                new_pos = final_txt.pos.apply(lambda x: ' '.join(x))\n",
    "                pos = self.pos_vectorizer.fit_transform(new_pos).todense()\n",
    "        return pos\n",
    "    \n",
    "    #extract tfidf from text and use svd for reducing the sparsity \n",
    "    def tfidf_ngrams_svd_text(self, ngram_range=(1,1), dimensionality = 2000):\n",
    "        self.word_tfidf = TfidfVectorizer(stop_words = 'english', ngram_range=ngram_range)\n",
    "        try:\n",
    "            tfs = self.word_tfidf.fit_transform(self.final_txt.text_lemmatized)\n",
    "        except AttributeError:\n",
    "            corpus = self.final_txt.text_lemmatized.apply(lambda x: ' '.join(x))\n",
    "            tfs = self.word_tfidf.fit_transform(corpus)\n",
    "        tfs = tfs.astype('float32')\n",
    "        \n",
    "        self.word_svdT = TruncatedSVD(n_components=dimensionality)\n",
    "        svdTFit = self.word_svdT.fit_transform(tfs)\n",
    "        return svdTFit\n",
    "        \n",
    "    def get_punctuaction(self):\n",
    "        punctuation_matrix = pd.json_normalize(self.final_txt[\"punctuation\"])\n",
    "        return punctuation_matrix\n",
    "    \n",
    "    def get_expres_initialism(self):\n",
    "        initialism_matrix = pd.json_normalize(self.final_txt['expressions_initialism'])\n",
    "        return initialism_matrix\n",
    "    \n",
    "    def get_expres_onomato(self):\n",
    "        onomato_matrix = pd.json_normalize(self.final_txt['expressions_onomato'])\n",
    "        return onomato_matrix\n",
    "    \n",
    "    def polarity_subjectivity_features(self):\n",
    "        polarity_subj_matrix = self.final_txt[['polarity','subjectivity']]\n",
    "        return polarity_subj_matrix\n",
    "    \n",
    "    def CountVect(self, col):\n",
    "        self.cv = CountVectorizer(analyzer=lambda x: x)\n",
    "        counted_values = self.cv.fit_transform(self.final_txt[col]).toarray()\n",
    "        df = pd.DataFrame(counted_values, columns=self.cv.get_feature_names())\n",
    "        return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '\\\\data\\\\SemEval2018-Task3\\\\datasets\\\\train\\\\SemEval2018-T3-train-taskA_emoji.txt'\n",
    "txt_file = read_csv(path, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file.rename({'Tweet text': 'text'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = preprocessing_text(txt_file,remove_hashtags=True, remove_mentions=True, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ddd.get_clean_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = ExtractFeatures(txt, 'pos_sequences', svd_transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, tfidfsvd_word_matrix, svdTFit_punctuation, svdTFit_emoji, svdTFit_onomato, svdTFit_initialism, polarity_subj_matrix = prova.get_all_features_train(ngram_range=(1,1), dimensionality=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
