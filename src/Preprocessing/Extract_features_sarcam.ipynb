{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FeaturesText\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/final_sarc_trainingset_twitter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = FeaturesText.preprocessing_text(data,remove_hashtags=True, remove_mentions=True, lowercase=True, arktweet_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_txt = txt_file.get_clean_df()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = train_txt[train_txt.astype(str).pos != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_txt = FeaturesText.ExtractFeatures(train_txt, 'other', svd_transform=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, punctuation_features, emoji_features, \\\n",
    "onomato_features, initialism_features,\\\n",
    "polarity_subj_features = final_train_txt.get_all_features_train(ngram_range=(1,1), dimensionality=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {'pos': pos,'polarity':polarity_subj_features, 'emoji': emoji_features,'punc': punctuation_features, \n",
    "                                'onom': onomato_features, 'init': initialism_features, 'label': np.asarray(train_txt.label.tolist())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert tweet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\",output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",  add_special_tokens=True,\n",
    "                                                max_length=70, pad_to_max_length=True,normalization=True, truncation=True, padding= True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [torch.tensor([tokenizer.encode(i, max_length=50, truncation=True)]) for i in train_txt.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(input_ids):\n",
    "        features = np.array(bertweet(i)[1]) #extract sentence embedding 1 x 768 for each document\n",
    "        features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_emb = np.array(features_list).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data_rescaled = scaler.fit_transform(bert_emb)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 0.95)\n",
    "pca.fit(data_rescaled)\n",
    "reduced = pca.transform(data_rescaled)\n",
    "import joblib \n",
    "joblib.dump(scaler, 'scaler_embed_sarcasm.pkl') \n",
    "joblib.dump(pca, 'pca_embedding_sarcasm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features['bert_embed'] = reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/features_training_sarc_twitter.p', 'wb') as fp:\n",
    "    pickle.dump(train_features, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riloff = pd.read_csv('../data/Riloff_twitter/riloff_sarc_train_test.csv')\n",
    "ghosh = pd.read_csv('../data/Ghosh_sarc_tweet/Test_v1.txt', sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghosh.rename({0: 'training', 1:'label', 2:'text'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt_file_riloff = FeaturesText.preprocessing_text(riloff,remove_hashtags=True, remove_mentions=True, lowercase=True, arktweet_pos=True)\n",
    "test_txt_file_ghosh = FeaturesText.preprocessing_text(ghosh,remove_hashtags=True, remove_mentions=True, lowercase=True, arktweet_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test_txt_ril = test_txt_file_riloff.get_clean_df()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test_txt_ghos = test_txt_file_ghosh.get_clean_df()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt_ril['text'] = test_txt_ril.text.str.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings Output Encoders BERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [torch.tensor([tokenizer.encode(i, truncation=True, max_length=70)]) for i in test_txt_ghos.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "batch_initial = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_middle = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_last = torch.zeros((len(input_ids),4,1,768))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        \n",
    "        sentence_emb_1 = torch.mean(features[2][1], dim=1).view(1, -1) #layer 1 \n",
    "        sentence_emb_2 = torch.mean(features[2][2], dim=1).view(1, -1)\n",
    "        sentence_emb_3 = torch.mean(features[2][3], dim=1).view(1, -1)\n",
    "        sentence_emb_4 = torch.mean(features[2][4], dim=1).view(1, -1)\n",
    "        sentence_emb_5 = torch.mean(features[2][5], dim=1).view(1, -1)\n",
    "        sentence_emb_6 = torch.mean(features[2][6], dim=1).view(1, -1)\n",
    "        sentence_emb_7 = torch.mean(features[2][7], dim=1).view(1, -1)\n",
    "        sentence_emb_8 = torch.mean(features[2][8], dim=1).view(1, -1)\n",
    "        sentence_emb_9 = torch.mean(features[2][9], dim=1).view(1, -1)\n",
    "        sentence_emb_10 = torch.mean(features[2][10], dim=1).view(1, -1)\n",
    "        sentence_emb_11 = torch.mean(features[2][11], dim=1).view(1, -1)\n",
    "        sentence_emb_12 = torch.mean(features[2][12], dim=1).view(1, -1) #layer 12\n",
    "        # B x C x H x W, 1 x 4 x 1 x 768\n",
    "        sub_layers_initial = torch.stack((sentence_emb_1, sentence_emb_2, sentence_emb_3, sentence_emb_4), dim= 1).reshape(1,4,1,768)  #add batch dimension\n",
    "        sub_layers_middle = torch.stack((sentence_emb_5, sentence_emb_6, sentence_emb_7, sentence_emb_8), dim= 1).reshape(1,4,1,768)\n",
    "        sub_layers_last = torch.stack((sentence_emb_9, sentence_emb_10, sentence_emb_11, sentence_emb_12), dim= 1).reshape(1,4,1,768)\n",
    "              \n",
    "        batch_initial[i,:] = sub_layers_initial\n",
    "        batch_middle[i,:] = sub_layers_middle\n",
    "        batch_last[i,:] = sub_layers_last\n",
    "        \n",
    "        y_test.append(test_txt_ghos.label.iloc[i])\n",
    "\n",
    "ground_test = torch.tensor(y_test, dtype = torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(batch_initial.float().clone(), '../data/new_approach/test/sarcasm/init_layer_riloff.pt')\n",
    "torch.save(batch_middle.float().clone(), '../data/new_approach/test/sarcasm/middle_layer_riloff.pt')\n",
    "torch.save(batch_last.float().clone(), '../data/new_approach/test/sarcasm/last_layer_riloff.pt')\n",
    "torch.save(ground_test.float().clone(), '../data/new_approach/test/sarcasm/y_riloff.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pos_rilof, punctuation_features_rilof, emoji_features_rilof, \\\n",
    "onomato_features_rilof, initialism_features_rilof,\\\n",
    "polarity_subj_features_rilof = FeaturesText.ExtractFeatures.get_all_features_test(final_train_txt, test_set=test_txt_ril)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ghosh, punctuation_features_ghosh, emoji_features_ghosh, \\\n",
    "onomato_features_ghosh, initialism_features_ghosh,\\\n",
    "polarity_subj_features_ghosh = FeaturesText.ExtractFeatures.get_all_features_test(final_train_txt, test_set=test_txt_ghos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_ril = {'pos': pos_rilof,'polarity':polarity_subj_features_rilof, 'emoji': emoji_features_rilof,'punc': punctuation_features_rilof, \n",
    "                                'onom': onomato_features_rilof, 'init': initialism_features_rilof, 'label': np.asarray(riloff.labels.tolist())}\n",
    "\n",
    "test_features_ghosh= {'pos': pos_ghosh,'polarity':polarity_subj_features_ghosh, 'emoji': emoji_features_ghosh,'punc': punctuation_features_ghosh, \n",
    "                                'onom': onomato_features_ghosh, 'init': initialism_features_ghosh, 'label': np.asarray(ghosh.label.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_ril = [torch.tensor([tokenizer.encode(i, max_length=50, truncation=True)]) for i in riloff.text]\n",
    "input_ids_ghosh = [torch.tensor([tokenizer.encode(i, max_length=50, truncation=True)]) for i in ghosh.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list_ril = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(input_ids_ril):\n",
    "        features = np.array(bertweet(i)[1]) #extract sentence embedding 1 x 768 for each document\n",
    "        features_list_ril.append(features)\n",
    "\n",
    "features_list_ghosh = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(input_ids_ghosh):\n",
    "        features = np.array(bertweet(i)[1]) #extract sentence embedding 1 x 768 for each document\n",
    "        features_list_ghosh.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_emb_ril = np.array(features_list_ril).squeeze()\n",
    "bert_emb_ghosh = np.array(features_list_ghosh).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later reload the pickle file\n",
    "import pickle as pk\n",
    "import joblib\n",
    "pca_reload = joblib.load(\"pca_embedding_sarcasm.pkl\")\n",
    "standar_reload = joblib.load(\"scaler_embed_sarcasm.pkl\")\n",
    "\n",
    " \n",
    "#result_new = pca_reload.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_ril = standar_reload.transform(bert_emb_ril)\n",
    "std_ghosh = standar_reload.transform(bert_emb_ghosh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_ril = pca_reload.transform(std_ril)\n",
    "reduced_ghosh = pca_reload.transform(std_ghosh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_ril['bert_embed'] = reduced_ril\n",
    "test_features_ghosh['bert_embed'] = reduced_ghosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/riloff_test_sarc.p', 'wb') as fp:\n",
    "    pickle.dump(test_features_ril, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('../data/ghosh_test_sarc.p', 'wb') as fp:\n",
    "    pickle.dump(test_features_ghosh, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
