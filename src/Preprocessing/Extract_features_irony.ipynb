{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FeaturesText\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/final_training_irony.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = FeaturesText.preprocessing_text(data,remove_hashtags=True, remove_mentions=True, lowercase=True, arktweet_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_txt = txt_file.get_clean_df()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = train_txt[train_txt.astype(str).pos != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_txt = FeaturesText.ExtractFeatures(train_txt, 'other', svd_transform=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, punctuation_features, emoji_features, \\\n",
    "onomato_features, initialism_features,\\\n",
    "polarity_subj_features = final_train_txt.get_all_features_train(ngram_range=(1,1), dimensionality=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {'pos': pos,'polarity':polarity_subj_features, 'emoji': emoji_features,'punc': punctuation_features, \n",
    "                                'onom': onomato_features, 'init': initialism_features, 'label': np.asarray(train_txt.label.tolist())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert tweet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",  add_special_tokens=True,\n",
    "                                                max_length=50, pad_to_max_length=True,normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [torch.tensor([tokenizer.encode(i, max_length=50, truncation=True)]) for i in train_txt.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(input_ids):\n",
    "        features = np.array(bertweet(i)[1]) #extract sentence embedding 1 x 768 for each document\n",
    "        features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_emb = np.array(features_list).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data_rescaled = scaler.fit_transform(bert_emb)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 0.95)\n",
    "pca.fit(data_rescaled)\n",
    "reduced = pca.transform(data_rescaled)\n",
    "import joblib \n",
    "joblib.dump(scaler, 'scaler_embed_irony.pkl') \n",
    "joblib.dump(pca, 'pca_embedding_irony.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features['bert_embed'] = reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/features_training_irony_twitter.p', 'wb') as fp:\n",
    "    pickle.dump(train_features, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem = pd.read_csv('../data/SemEval2018-Task3/datasets/goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem.rename({'Tweet text': 'text', 'Label' : 'label'}, axis = 1, inplace=True)\n",
    "sem = pd.read_csv('../data/SemEval2018-Task3/datasets/goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt= FeaturesText.preprocessing_text(sem,remove_hashtags=True, remove_mentions=True, lowercase=True, arktweet_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test_txt_sem = test_txt.get_clean_df()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sem, punctuation_features_sem, emoji_features_sem, \\\n",
    "onomato_features_sem, initialism_features_sem,\\\n",
    "polarity_subj_features_sem = FeaturesText.ExtractFeatures.get_all_features_test(final_train_txt, test_set=test_txt_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_sem = {'pos': pos_sem,'polarity':polarity_subj_features_sem, 'emoji': emoji_features_sem,'punc': punctuation_features_sem, \n",
    "                                'onom': onomato_features_sem, 'init': initialism_features_sem, 'label': np.asarray(sem.label.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_sem = [torch.tensor([tokenizer.encode(i, max_length=50, truncation=True)]) for i in sem.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list_sem = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(input_ids_sem):\n",
    "        features = np.array(bertweet(i)[1]) #extract sentence embedding 1 x 768 for each document\n",
    "        features_list_sem.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_emb_sem = np.array(features_list_sem).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later reload the pickle file\n",
    "import pickle as pk\n",
    "import joblib\n",
    "pca_reload = joblib.load(\"pca_embedding_irony.pkl\")\n",
    "standar_reload = joblib.load(\"scaler_embed_irony.pkl\")\n",
    "\n",
    " \n",
    "#result_new = pca_reload.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sem = standar_reload.transform(bert_emb_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_sem = pca_reload.transform(std_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_sem['bert_embed'] = reduced_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/semeval_test3a_irony.p', 'wb') as fp:\n",
    "    pickle.dump(test_features_sem, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
