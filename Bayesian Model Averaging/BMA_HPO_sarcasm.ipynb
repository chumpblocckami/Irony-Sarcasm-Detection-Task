{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/features_training_sarc_twitter.p', 'rb') as handle:\n",
    "    train = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pos'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed = train['bert_embed']\n",
    "\n",
    "X_train_pp = np.concatenate([train['bert_embed'], train['emoji']['emoji'],\n",
    "                             np.expand_dims(train['emoji']['emoji_positive'], axis = 1), np.expand_dims(train['emoji']['emoji_negative'], axis = 1),\n",
    "                         train['punc'], train['onom'], train['init']], axis = 1)\n",
    "\n",
    "X_train_pos = np.concatenate([train['pos'], train['bert_embed']], axis = 1)\n",
    "\n",
    "X_train_pp_pos = np.concatenate([train['emoji']['emoji'],np.expand_dims(train['emoji']['emoji_positive'], axis = 1), \n",
    "                                 np.expand_dims(train['emoji']['emoji_negative'], axis = 1), train['pos'],train['punc'],\n",
    "                                 train['onom'], train['init'], train['bert_embed']], axis = 1)\n",
    "\n",
    "X_train_pp_pos_pol = np.concatenate([train['emoji']['emoji'],np.expand_dims(train['emoji']['emoji_positive'], axis = 1), \n",
    "                                 np.expand_dims(train['emoji']['emoji_negative'], axis = 1), train['pos'],train['punc'],\n",
    "                                 train['onom'], train['init'], train['bert_embed'], train['polarity']], axis = 1)\n",
    "\n",
    "y_train = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pol = np.concatenate([train['bert_embed'], train['polarity']], axis =1)\n",
    "X_train_pos_pol = np.concatenate([train['bert_embed'], train['polarity'], train['pos']], axis = 1)\n",
    "X_train_pp_pol = np.concatenate([train['bert_embed'], train['emoji']['emoji'],\n",
    "                             np.expand_dims(train['emoji']['emoji_positive'], axis = 1), np.expand_dims(train['emoji']['emoji_negative'], axis = 1),\n",
    "                         train['punc'], train['onom'], train['init']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# top_v = 0 \n",
    "\n",
    "# def change_max(x):\n",
    "#     global top_v\n",
    "#     if x >= top_v:\n",
    "#         top_v = x\n",
    "#         return x\n",
    "#     else:\n",
    "#         return top_v\n",
    "\n",
    "# def random_searchcv(X_train, y_train, clf,parameters, cv = 5, metric = 'accuracy', iter = 25):\n",
    "\n",
    "#     pipeline = Pipeline([\n",
    "#        # ('sampling', SMOTE(random_state=42)),\n",
    "#         ('classification',clf)])\n",
    "#     random_grid = {'classification__' + key: parameters[key] for key in parameters}\n",
    "#     strat = StratifiedKFold(n_splits = cv, random_state = None)\n",
    "\n",
    "#     clf_randomsearch = RandomizedSearchCV(pipeline, random_grid,n_iter = iter, cv = strat, scoring = metric,  n_jobs=-1)\n",
    "#     clf_randomsearch.fit(X_train, y_train)\n",
    "\n",
    "#     print(\"Best configuration: {}\\n Metric score: {}\".format(clf_randomsearch.best_params_, clf_randomsearch.best_score_))\n",
    "\n",
    "#     result_random_search = clf_randomsearch.cv_results_['mean_test_score']\n",
    "\n",
    "#     best_seen = [change_max(i) for i in result_random_search]\n",
    "\n",
    "#     return clf_randomsearch, best_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "# Define dictionary with performance metrics\n",
    "scoring = {'accuracy':make_scorer(accuracy_score), \n",
    "           'precision':make_scorer(precision_score),\n",
    "           'recall':make_scorer(recall_score), \n",
    "           'f1_score':make_scorer(f1_score)}\n",
    "\n",
    "# Import required libraries for machine learning classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy import stats\n",
    "# Instantiate the machine learning classifiers\n",
    "log_model = LogisticRegression(max_iter=500)\n",
    "dtr_model = HistGradientBoostingClassifier()\n",
    "rfc_model = RandomForestClassifier()\n",
    "gnb_model = AdaBoostClassifier()\n",
    "xgb_model = xgb.XGBClassifier(objective = 'binary:logistic') #tree_method = 'gpu_hist'\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Define the models evaluation function\n",
    "def models_evaluation(X, y, folds, epoch, metric = 'accuracy'):\n",
    "    \n",
    "    '''\n",
    "    X : data set features\n",
    "    y : data set target\n",
    "    folds : number of cross-validation folds\n",
    "    \n",
    "    '''\n",
    "    diz = {}\n",
    "    rand_list_xgb = {'n_estimators': stats.randint(200, 500),\n",
    "              'learning_rate': stats.uniform(0.01, 0.6),\n",
    "              'subsample': stats.uniform(0.3, 0.9),\n",
    "              'max_depth': stats.randint(3, 30),\n",
    "              'min_child_weight':stats.randint(1, 20)\n",
    "             }\n",
    "    rand_list_svm = {\"C\": stats.uniform(2, 20),\"gamma\": stats.uniform(0.1, 1), 'kernel': ['linear', 'rbf', 'sigmoid']}\n",
    "    rand_list_reg = { 'C': stats.uniform(0.1, 10), 'penalty' : ['l2'], 'solver' : ['liblinear', 'saga']}\n",
    "    rand_list_hist = {'max_depth': stats.randint(3, 30), 'min_samples_leaf': stats.randint(1, 20), 'learning_rate': stats.uniform(0.001, 0.1)}\n",
    "    rand_list_rf = {'max_depth': stats.randint(3, 30), 'min_samples_leaf':  stats.randint(1, 20), 'min_samples_split': stats.randint(1, 20), 'n_estimators':  stats.randint(50, 500)}\n",
    "    rand_list_ada = {'n_estimators':  stats.randint(50, 500), 'learning_rate' : stats.uniform(0.001, 0.1)}\n",
    "    \n",
    "    strat = StratifiedKFold(n_splits = folds, random_state = None)\n",
    "    \n",
    "    clf_randomsearch_hgb = RandomizedSearchCV(dtr_model, rand_list_hist, n_iter = epoch, cv = strat, scoring = metric,  n_jobs=6, verbose = 2)\n",
    "    clf_randomsearch_hgb.fit(X, y)\n",
    "    diz['Hgboost'] = get_val_metrics(clf_randomsearch_hgb)\n",
    "    std = clf_randomsearch_hgb.cv_results_['std_test_score'][clf_randomsearch_hgb.best_index_]\n",
    "    print(\"Best configuration for Hist G.Boost: {}\\n Metric score: {} and std score: {}\".format(clf_randomsearch_hgb.best_params_, clf_randomsearch_hgb.best_score_, std))\n",
    "    \n",
    "    clf_randomsearch_rf = RandomizedSearchCV(rfc_model, rand_list_rf, n_iter = epoch, cv = strat, scoring = metric,  n_jobs=6, verbose = 2)\n",
    "    clf_randomsearch_rf.fit(X, y)\n",
    "    std = clf_randomsearch_rf.cv_results_['std_test_score'][clf_randomsearch_rf.best_index_]\n",
    "    diz['RandomForest'] = get_val_metrics(clf_randomsearch_rf)\n",
    "    print(\"Best configuration for Random Forest: {}\\n Metric score: {} and std score: {}\".format(clf_randomsearch_rf.best_params_, clf_randomsearch_rf.best_score_, std))\n",
    "    \n",
    "    clf_randomsearch_xgb = RandomizedSearchCV(xgb_model, rand_list_xgb, n_iter = epoch, cv = strat, scoring = metric,  n_jobs=6, verbose = 2)\n",
    "    clf_randomsearch_xgb.fit(X, y)\n",
    "    diz['XgBoost'] = get_val_metrics(clf_randomsearch_xgb)\n",
    "    std = clf_randomsearch_xgb.cv_results_['std_test_score'][clf_randomsearch_xgb.best_index_]\n",
    "    print(\"Best configuration for XG.Boost: {}\\n Metric score: {} and std score: {}\".format(clf_randomsearch_xgb.best_params_, clf_randomsearch_xgb.best_score_, std))\n",
    "    \n",
    "    clf_randomsearch_ada = RandomizedSearchCV(gnb_model, rand_list_ada, n_iter = epoch, cv = strat, scoring = metric,  n_jobs=6, verbose = 2)\n",
    "    clf_randomsearch_ada.fit(X, y)\n",
    "    diz['AdaBoost'] = get_val_metrics(clf_randomsearch_ada)\n",
    "    std = clf_randomsearch_ada.cv_results_['std_test_score'][clf_randomsearch_ada.best_index_]\n",
    "    print(\"Best configuration for Ada Boost: {}\\n Metric score: {} and std score: {}\".format(clf_randomsearch_ada.best_params_, clf_randomsearch_ada.best_score_, std))\n",
    "    \n",
    "    clf_randomsearch_reg = RandomizedSearchCV(log_model, rand_list_reg, n_iter = epoch, cv = strat, scoring = metric,  n_jobs=6, verbose = 2)\n",
    "    clf_randomsearch_reg.fit(X, y)\n",
    "    diz['Logistic'] = get_val_metrics(clf_randomsearch_reg)\n",
    "    std = clf_randomsearch_reg.cv_results_['std_test_score'][clf_randomsearch_reg.best_index_]\n",
    "    print(\"Best configuration for Logistic Regression: {}\\n Metric score: {} and std score: {}\".format(clf_randomsearch_reg.best_params_, clf_randomsearch_reg.best_score_, std))\n",
    "    \n",
    "    return diz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_metrics(clf): \n",
    "    split0 = clf.cv_results_['split0_test_score'][clf.best_index_]\n",
    "    split1 = clf.cv_results_['split1_test_score'][clf.best_index_]\n",
    "    split2 = clf.cv_results_['split2_test_score'][clf.best_index_]\n",
    "    split3 = clf.cv_results_['split3_test_score'][clf.best_index_]\n",
    "    split4 = clf.cv_results_['split4_test_score'][clf.best_index_]\n",
    "    split5 = clf.cv_results_['split5_test_score'][clf.best_index_]\n",
    "    fold_results = [split0,split1, split2, split3, split4, split5]\n",
    "    \n",
    "    return fold_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Embedding features')\n",
    "score = models_evaluation(X_train_embed, y_train, 6, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PP features\")\n",
    "score2 = models_evaluation(X_train_pp, y_train, 6, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pos features\")\n",
    "score3 = models_evaluation(X_train_pos, y_train, 6, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pos + PP features\")\n",
    "score4 = models_evaluation(X_train_pp_pos, y_train, 6, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pos + PP features + Polarity\")\n",
    "score5 = models_evaluation(X_train_pp_pos_pol, y_train, 6, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Polarity')\n",
    "score6 = models_evaluation(X_train_pol, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pos and Polarity')\n",
    "score7 = models_evaluation(X_train_pos_pol, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PP and Polarity')\n",
    "score8 = models_evaluation(X_train_pp_pol, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_score = [score6,score7,score8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = [\"POL\", \"POS+POL\", \"PP+POL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_score = [score,score2,score3, score4, score5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = [\"Embedding\",\"PP\",\"POS\",\"POS+PP\",\"POS+PP+POL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_scores = {}\n",
    "for i in range(len(list_score)):\n",
    "    diz_scores['{}'.format(features_name[i])] = list_score[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('random_search_sarcasm_paper.p', 'wb') as fp:\n",
    "    pickle.dump(diz_scores, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_search_sarcasm_paper.p', 'rb') as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(samples, n_bootstrap, size_samples, ic = 0.95): #size sample n splits \n",
    "    diz = {}\n",
    "    samples = np.array(list(samples))\n",
    "    x_mean = np.mean(samples)\n",
    "    samples_boot = []\n",
    "    for i in range(n_bootstrap):\n",
    "        samples_boot.append(np.mean(np.random.choice(np.squeeze(samples), size_samples)))\n",
    "    scarti = samples_boot - x_mean\n",
    "    v = 100 - ic*100\n",
    "    pinf = 100 - v/2\n",
    "    psup = 100 - pinf\n",
    "    lim_inf = x_mean - np.percentile(scarti, pinf)\n",
    "    lim_sup = x_mean - np.percentile(scarti, psup)\n",
    "    diz['Mean'] = x_mean\n",
    "    diz['Lower'] = lim_inf\n",
    "    diz['Upper'] = lim_sup\n",
    "    return diz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_boostrap(df, column):    \n",
    "    diz = {}\n",
    "    diz2 = {}\n",
    "    diz['Hgboost'] = bootstrap(df[df['index'] == 'Hgboost'][column], 50, 6, 0.95)\n",
    "    diz['RandomForest'] = bootstrap(df[df['index'] == 'RandomForest'][column], 50, 6, 0.95)\n",
    "    diz['XgBoost']  = bootstrap(df[df['index'] == 'XgBoost'][column], 50, 6, 0.95)\n",
    "    diz['AdaBoost'] = bootstrap(df[df['index'] == 'AdaBoost'][column], 50, 6, 0.95)\n",
    "    diz['Logistic'] = bootstrap(df[df['index'] == 'Logistic'][column], 50, 6, 0.95)\n",
    "    return pd.DataFrame(diz).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = compute_boostrap(data, \"Embedding\").reset_index()\n",
    "embed2 = compute_boostrap(data, \"PP\").reset_index()\n",
    "embed3 = compute_boostrap(data, \"POS\").reset_index()\n",
    "embed4 = compute_boostrap(data, \"POS+PP\").reset_index()\n",
    "embed5 = compute_boostrap(data, \"POS+PP+POL\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed['Features'] = 'Embedding'\n",
    "embed2['Features'] = 'PP'\n",
    "embed3['Features'] = 'POS'\n",
    "embed4['Features'] = 'POS+PP'\n",
    "embed5['Features'] = 'POS+PP+POL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = pd.concat([embed, embed2, embed3, embed4, embed5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = final_score.rename({'index':'Model'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score.to_csv('../data/Optimization/Sarcasm/random_search.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "def bayes_opt(X_train, y_train, fold, n_iter):\n",
    "    diz = {}\n",
    "    global count \n",
    "    \n",
    "    log_model = LogisticRegression(max_iter=500)\n",
    "    dtr_model = HistGradientBoostingClassifier()\n",
    "    rfc_model = RandomForestClassifier()\n",
    "    ada_model = AdaBoostClassifier()\n",
    "    xgb_model = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "\n",
    "\n",
    "    rand_list_xgb = {'n_estimators': [int(x) for x in np.linspace(50, 300, num=251)],\n",
    "              'learning_rate':  np.linspace(1e-3, 1, num=500),\n",
    "              'subsample': [np.random.uniform(0.3, 0.9) for _ in range(200)],\n",
    "              'max_depth': list(range(3,21)),\n",
    "              'colsample_bytree': [np.random.uniform(0.5, 0.9) for _ in range(200)],\n",
    "              'min_child_weight':list(range(1,21))\n",
    "             }\n",
    "    rand_list_reg = { 'C': [np.random.uniform(0.1, 10) for _ in range(200)], 'penalty' : ['l2']}\n",
    "    rand_list_hist = {'max_depth': list(range(3,21)), 'min_samples_leaf': list(range(1,21)), 'learning_rate': np.linspace(1e-3, 1, num=500)}\n",
    "    rand_list_rf = {'max_depth': list(range(3,21)), 'min_samples_leaf':  list(range(1,21)), 'min_samples_split': list(range(2,21)), 'n_estimators':   [int(x) for x in np.linspace(50, 300, num=251)]}\n",
    "    rand_list_ada = {'n_estimators':   [int(x) for x in np.linspace(50, 300, num=251)], 'learning_rate':  np.linspace(1e-3, 1, num=500)}\n",
    "\n",
    "\n",
    "    # rand_list_xgb = {'xgbclassifier__' + key: rand_list_xgb[key] for key in rand_list_xgb}\n",
    "    # rand_list_reg = {'logisticregression__' + key: rand_list_reg[key] for key in rand_list_reg}\n",
    "    # rand_list_hist = {'histgradientboostingclassifier__' + key: rand_list_hist[key] for key in rand_list_hist}\n",
    "    # rand_list_rf = {'randomforestclassifier__' + key: rand_list_rf[key] for key in rand_list_rf}\n",
    "    # rand_list_ada = {'adaboostclassifier__'  + key: rand_list_ada[key] for key in rand_list_ada}\n",
    "\n",
    "    gb_bayes_reglog = BayesSearchCV(log_model, rand_list_reg, n_iter=n_iter, cv=fold,\n",
    "                             random_state=1, n_jobs=6, refit=True)\n",
    "    \n",
    "    gb_bayes_xgb = BayesSearchCV(xgb_model, rand_list_xgb, n_iter=n_iter, cv=fold,\n",
    "                             random_state=1, n_jobs=6, refit=True)\n",
    "\n",
    "    gb_bayes_hist = BayesSearchCV(dtr_model, rand_list_hist, n_iter=n_iter, cv=fold,\n",
    "                             random_state=1, n_jobs=6, refit=True)\n",
    "\n",
    "    gb_bayes_rf = BayesSearchCV(rfc_model, rand_list_rf, n_iter=n_iter, cv=fold,\n",
    "                             random_state=1, n_jobs=6, refit=True)\n",
    "\n",
    "    gb_bayes_ada = BayesSearchCV(ada_model, rand_list_ada, n_iter=n_iter, cv=fold,\n",
    "                            random_state=1, n_jobs=6, refit=True)\n",
    "\n",
    "    count = 1\n",
    "    def on_epoch(optim_result):\n",
    "        global count\n",
    "        if count == n_iter:\n",
    "            std = gb_bayes_reglog.cv_results_['std_test_score'][gb_bayes_reglog.best_index_]\n",
    "            print(\"Params:\",gb_bayes_reglog.best_params_, \" Logistic regression score:\",gb_bayes_reglog.best_score_,' standard dev: ', std)    \n",
    "        count += 1\n",
    "        \n",
    "\n",
    "    gb_bayes_reglog.fit(X_train, y_train, callback = on_epoch)\n",
    "    diz['Logistic'] = get_val_metrics(gb_bayes_reglog)\n",
    "    count = 1\n",
    "    def on_epoch(optim_result):\n",
    "        global count\n",
    "        if count == n_iter:\n",
    "            std = gb_bayes_xgb.cv_results_['std_test_score'][gb_bayes_xgb.best_index_]\n",
    "            print(\"Params:\",gb_bayes_xgb.best_params_, \" Xgboost score:\",gb_bayes_xgb.best_score_,' standard dev: ', std)    \n",
    "        count += 1\n",
    "    gb_bayes_xgb.fit(X_train, y_train, callback = on_epoch)\n",
    "    diz['XgBoost'] = get_val_metrics(gb_bayes_xgb)\n",
    "    count = 1\n",
    "    def on_epoch(optim_result):\n",
    "        global count\n",
    "        if count == n_iter:\n",
    "            std = gb_bayes_hist.cv_results_['std_test_score'][gb_bayes_hist.best_index_]\n",
    "            print(\"Params:\",gb_bayes_hist.best_params_, \"Hist GB score:\",gb_bayes_hist.best_score_,' standard dev: ', std)    \n",
    "        count += 1\n",
    "    gb_bayes_hist.fit(X_train, y_train, callback = on_epoch)\n",
    "    diz['Hgboost'] = get_val_metrics(gb_bayes_hist)\n",
    "    count = 1\n",
    "    def on_epoch(optim_result):\n",
    "        global count\n",
    "        if count == n_iter:\n",
    "            std = gb_bayes_rf.cv_results_['std_test_score'][gb_bayes_rf.best_index_]\n",
    "            print(\"Params:\",gb_bayes_rf.best_params_, \" Random Forest score:\",gb_bayes_rf.best_score_,' standard dev: ', std)    \n",
    "        count += 1\n",
    "    gb_bayes_rf.fit(X_train, y_train, callback = on_epoch)\n",
    "    diz['RandomForest'] = get_val_metrics(gb_bayes_rf)\n",
    "    count = 1\n",
    "    def on_epoch(optim_result):\n",
    "        global count\n",
    "        if count == n_iter:\n",
    "            std = gb_bayes_ada.cv_results_['std_test_score'][gb_bayes_ada.best_index_]\n",
    "            print(\"Params:\",gb_bayes_ada.best_params_, \" Ada Boost score:\",gb_bayes_ada.best_score_,' standard dev: ', std)    \n",
    "        count += 1\n",
    "    gb_bayes_ada.fit(X_train, y_train, callback = on_epoch)\n",
    "    diz['Adaboost'] = get_val_metrics(gb_bayes_ada)\n",
    "    return diz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Embedding features')\n",
    "score = bayes_opt(X_train_embed, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PP features\")\n",
    "score2 = bayes_opt(X_train_pp, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pos features\")\n",
    "score3 = bayes_opt(X_train_pos, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pos + PP features\")\n",
    "score4 = bayes_opt(X_train_pp_pos, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pos + PP features + Polarity\")\n",
    "score5 = bayes_opt(X_train_pp_pos_pol, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score6 = bayes_opt(X_train_pol, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score7 = bayes_opt(X_train_pos_pol, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score8 =  bayes_opt(X_train_pp_pol, y_train, 6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_scores = {}\n",
    "for i in range(len(list_score)):\n",
    "    diz_scores['{}'.format(features_name[i])] = list_score[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('bayesian_search_sarcasm_paper.p', 'wb') as fp:\n",
    "    pickle.dump(diz_scores, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(diz_scores).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"POL\", \"POS+POL\", \"PP+POL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = compute_boostrap(data, \"POL\").reset_index()\n",
    "embed2 = compute_boostrap(data, \"POS+POL\").reset_index()\n",
    "embed3 = compute_boostrap(data, \"PP+POL\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = compute_boostrap(data, \"Embedding\").reset_index()\n",
    "embed2 = compute_boostrap(data, \"PP\").reset_index()\n",
    "embed3 = compute_boostrap(data, \"POS\").reset_index()\n",
    "embed4 = compute_boostrap(data, \"POS+PP\").reset_index()\n",
    "embed5 = compute_boostrap(data, \"POS+PP+POL\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed['Features'] = 'POL'\n",
    "embed2['Features'] = 'POS+POL'\n",
    "embed3['Features'] = 'PP+POL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed['Features'] = 'Embedding'\n",
    "embed2['Features'] = 'PP'\n",
    "embed3['Features'] = 'POS'\n",
    "embed4['Features'] = 'POS+PP'\n",
    "embed5['Features'] = 'POS+PP+POL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = pd.concat([embed, embed2, embed3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = final_score.rename({'index':'Model'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score.to_csv('../data/Optimization/Sarcasm/bayesian_search_otherfeatures.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BMA JAVA INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lab(x):\n",
    "    new_lab_list = []\n",
    "    for i in x:\n",
    "        if i == 0:\n",
    "            new_lab = '1:1'\n",
    "        else:\n",
    "            new_lab =  '2:0'\n",
    "            \n",
    "        new_lab_list.append(new_lab)\n",
    "        \n",
    "    return new_lab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_error(actual, predicted):\n",
    "    mark_list = []\n",
    "    for i,j in zip(actual, predicted):\n",
    "        if i != j:\n",
    "            mark = '+'\n",
    "        else:\n",
    "            mark = np.nan\n",
    "            \n",
    "        mark_list.append(mark)\n",
    "        \n",
    "    return mark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proba_distrib(clf_proba):\n",
    "    proba_ast = []\n",
    "    for i,j in zip(clf_proba[:,0], clf_proba[:,1]):\n",
    "        if i > j:\n",
    "            proba = ['*{}'.format(str(i.round(5))), str(j.round(5))]\n",
    "        else:\n",
    "            proba = [str(i.round(5)), '*{}'.format(str(j.round(5)))]\n",
    "            \n",
    "        proba_ast.append(proba)\n",
    "        \n",
    "    return np.array(proba_ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outpupt_bma(clf, x, ground_truth):\n",
    "    \n",
    "    pred = clf.predict(x)\n",
    "    \n",
    "    actual = normalize_lab(ground_truth)\n",
    "    predicted = normalize_lab(pred)\n",
    "    \n",
    "    error = mark_error(actual, predicted)\n",
    "    conta = 0\n",
    "    lista_ins = []\n",
    "    for i in range(len(x)):\n",
    "        conta += 1\n",
    "        if conta == int(len(x)/10) + 2:\n",
    "            conta = 1\n",
    "     \n",
    "        lista_ins.append(conta)  \n",
    "        \n",
    "    instanc = lista_ins\n",
    "    \n",
    "    predict_proba = clf.predict_proba(x)\n",
    "    \n",
    "    distribution = get_proba_distrib(predict_proba)\n",
    "    \n",
    "    final_df = pd.DataFrame(instanc, columns=['inst#'])\n",
    "    \n",
    "    final_df['actual'] = actual\n",
    "    \n",
    "    final_df['predicted'] = predicted\n",
    "    \n",
    "    final_df['error'] = error\n",
    "\n",
    "    final_df['distribution'] = distribution[:, 0]\n",
    "    \n",
    "    final_df[''] = distribution[:,1]\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models for sarcasm with the best hyperparameters identified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(colsample_bytree = 0.7776353921686654, learning_rate =  0.063062124248497, max_depth= 9, min_child_weight= 4, n_estimators= 173, subsample= 0.834149882785828) #pos pp pola bayes\n",
    "randomf_model = RandomForestClassifier(max_depth = 18, min_samples_leaf = 8, min_samples_split = 2, n_estimators = 193) #pos, polarity, pp bayes\n",
    "hist_model =  HistGradientBoostingClassifier(learning_rate = 0.09137860709617293,max_depth = 23, min_samples_leaf = 16) #pos, polarity, pp random\n",
    "logi_model = LogisticRegression(C =  6.3851824328733695, penalty = 'l2')  #pos, polarity, bayes\n",
    "ada_model = AdaBoostClassifier(learning_rate = 0.9679679358717436, n_estimators = 158) # pos pp bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train_pp_pos_pol, y_train)\n",
    "randomf_model.fit(X_train_pp_pos_pol, y_train)\n",
    "hist_model.fit(X_train_pp_pos_pol, y_train)\n",
    "logi_model.fit(X_train_pp_pos_pol, y_train)\n",
    "ada_model.fit(X_train_pp_pos, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sarcasm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/ghosh_test_sarc.p', 'rb') as handle:\n",
    "    ghosh_test = pickle.load(handle)\n",
    "with open('../data/riloff_test_sarc.p', 'rb') as handle:\n",
    "    riloff_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghosh_pos = np.concatenate([ghosh_test['pos'], ghosh_test['bert_embed']], axis = 1)\n",
    "ghosh_pp_pos_pol = np.concatenate([ghosh_test['emoji']['emoji'],np.expand_dims(ghosh_test['emoji']['emoji_positive'], axis = 1), \n",
    "                                 np.expand_dims(ghosh_test['emoji']['emoji_negative'], axis = 1), ghosh_test['pos'],ghosh_test['punc'],\n",
    "                                 ghosh_test['onom'], ghosh_test['init'], ghosh_test['bert_embed'], ghosh_test['polarity']], axis = 1)\n",
    "ghosh_pp_pos = np.concatenate([ghosh_test['emoji']['emoji'],np.expand_dims(ghosh_test['emoji']['emoji_positive'], axis = 1), \n",
    "                                 np.expand_dims(ghosh_test['emoji']['emoji_negative'], axis = 1), ghosh_test['pos'],ghosh_test['punc'],\n",
    "                                 ghosh_test['onom'], ghosh_test['init'], ghosh_test['bert_embed']], axis = 1)\n",
    "\n",
    "y_ghosh = ghosh_test['label']\n",
    "\n",
    "riloff_pos =  np.concatenate([riloff_test['pos'], riloff_test['bert_embed']], axis = 1)\n",
    "riloff_pp_pos_pol = np.concatenate([riloff_test['emoji']['emoji'],np.expand_dims(riloff_test['emoji']['emoji_positive'], axis = 1), \n",
    "                                 np.expand_dims(riloff_test['emoji']['emoji_negative'], axis = 1), riloff_test['pos'],riloff_test['punc'],\n",
    "                                 riloff_test['onom'], riloff_test['init'], riloff_test['bert_embed'], riloff_test['polarity']], axis = 1)\n",
    "\n",
    "riloff_pp_pos = np.concatenate([riloff_test['emoji']['emoji'],np.expand_dims(riloff_test['emoji']['emoji_positive'], axis = 1), \n",
    "                                 np.expand_dims(riloff_test['emoji']['emoji_negative'], axis = 1), riloff_test['pos'],riloff_test['punc'],\n",
    "                                 riloff_test['onom'], riloff_test['init'], riloff_test['bert_embed']], axis = 1)\n",
    "\n",
    "y_riloff = riloff_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_ghosh, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = pd.concat([embed, embed2, embed3, embed4, embed5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_output = get_outpupt_bma(xgb_model, ghosh_pp_pos_pol, y_ghosh)\n",
    "hist_output = get_outpupt_bma(hist_model, ghosh_pp_pos_pol, y_ghosh)\n",
    "rf_output = get_outpupt_bma(randomf_model, ghosh_pp_pos_pol, y_ghosh)\n",
    "ada_output = get_outpupt_bma(ada_model, ghosh_pp_pos, y_ghosh)\n",
    "logi_output = get_outpupt_bma(logi_model, ghosh_pp_pos_pol, y_ghosh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_output.to_csv('../Code/BMA/results_ghosh/input/prediction_file/xgb_labels.csv', index = False)\n",
    "hist_output.to_csv('../Code/BMA/results_ghosh/input/prediction_file/hist_labels.csv', index = False)\n",
    "rf_output.to_csv('../Code/BMA/results_ghosh/input/prediction_file/rf_labels.csv', index = False)\n",
    "ada_output.to_csv('../Code/BMA/results_ghosh/input/prediction_file/ada_labels.csv', index = False)\n",
    "logi_output.to_csv('../Code/BMA/results_ghosh/input/prediction_file/logi_labels.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_output = get_outpupt_bma(xgb_model, riloff_pp_pos_pol, y_riloff)\n",
    "hist_output = get_outpupt_bma(hist_model, riloff_pp_pos_pol, y_riloff)\n",
    "rf_output = get_outpupt_bma(randomf_model, riloff_pp_pos_pol, y_riloff)\n",
    "ada_output = get_outpupt_bma(ada_model, riloff_pp_pos, y_riloff)\n",
    "logi_output = get_outpupt_bma(logi_model, riloff_pp_pos_pol, y_riloff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ghosh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_output.to_csv('../Code/BMA/results_riloff/input/prediction_file/xgb_labels.csv', index = False)\n",
    "hist_output.to_csv('../Code/BMA/results_riloff/input/prediction_file/hist_labels.csv', index = False)\n",
    "rf_output.to_csv('../Code/BMA/results_riloff/input/prediction_file/rf_labels.csv', index = False)\n",
    "ada_output.to_csv('../Code/BMA/results_riloff/input/prediction_file/ada_labels.csv', index = False)\n",
    "logi_output.to_csv('../Code/BMA/results_riloff/input/prediction_file/logi_labels.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(hist_model.predict(X_train_pp_pos_pol), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(xgb_model.predict(ghosh_pp_pos_pol), y_ghosh))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
