{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer \n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from torch import nn\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from ark_tweet_pos import CMUTweetTagger\n",
    "import shlex\n",
    "run_tagger_cmd = \"java -XX:ParallelGCThreads=10 -Xmx500m -jar ark_tweet_pos/ark-tweet-nlp-0.3.2.jar\"\n",
    "import FeaturesText\n",
    "import wandb\n",
    "wandb.login()\n",
    "#f916c4a8279e06f6c75fb2a86d88784b94e8a539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\",output_hidden_states=True)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'D:/Data_Science_all/MSC_2_anno/Tesi_Irony_Sarcasm/data/final_sarc_trainingset_twitter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract pos and embeddings layers from bert-tweet (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.replace(r'#([^\\s:]+)', '')\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = FeaturesText.preprocessing_text(data,remove_hashtags=True, remove_mentions=True, lowercase=True, arktweet_pos=True)\n",
    "start = time.time()\n",
    "train_txt = txt_file.get_clean_df()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = train_txt[train_txt.astype(str).pos != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_txt = FeaturesText.ExtractFeatures(train_txt, 'other', svd_transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, punctuation_features, emoji_features, \\\n",
    "onomato_features, initialism_features,\\\n",
    "polarity_subj_features = final_train_txt.get_all_features_train(ngram_range=(1,1), dimensionality=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {'pos': pos,'polarity':polarity_subj_features, 'emoji': emoji_features,'punc': punctuation_features, \n",
    "                                'onom': onomato_features, 'init': initialism_features, 'label': np.asarray(train_txt.label.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/features_training_sarc_twitter_new_approach.p', 'wb') as fp:\n",
    "    pickle.dump(train_features, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:/Data_Science_all/MSC_2_anno/Tesi_Irony_Sarcasm/data/final_sarc_trainingset_twitter_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['list'] = df.text.apply(lambda x: x.split(' '))\n",
    "df['len_list'] = df.list.str.len()\n",
    "df = df[df.len_list > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = np.array([len(ast.literal_eval(i)) for i in df.pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=30)\n",
    "tokenizer.fit_on_texts(train_txt['pos'].astype(str))\n",
    "sequences_pos = tokenizer.texts_to_sequences(train_txt['pos'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = pad_sequences(sequences_pos, maxlen=30, padding='post', truncating='post')\n",
    "pos_tensor = torch.unsqueeze(torch.tensor(data_pos, dtype=torch.float),1)\n",
    "torch.save(pos_tensor.float().clone(), '../data/new_approach/train/sarcasm/pos_tensor.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.augmentation import WordNetAugmenter\n",
    "from textattack.augmentation import EmbeddingAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = WordNetAugmenter(pct_words_to_swap=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "text_aug = []\n",
    "for i in tqdm(range(len(df))): \n",
    "        if i >= 7000:\n",
    "            aug = augmenter.augment(df.text.iloc[i])\n",
    "            label = df.label.iloc[i]\n",
    "            text_aug.append(' '.join(map(str, aug)))\n",
    "            label_list.append(label)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = pd.DataFrame(text_aug, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['label'] = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = pd.concat([df, augmented]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt.to_csv('../data/new_approach/augmented_sarcasm_training.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Sentence embedding extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt.drop('index', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [torch.tensor([tokenizer_bert.encode(i)]) for i in df.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentence = torch.zeros((len(input_ids),1,768))\n",
    "y_target = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        batch_sentence[i, :] = features[1]\n",
    "        y_target.append(train_txt.label.iloc[i])\n",
    "        \n",
    "ground_truth = torch.tensor(y_target, dtype = torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(batch_sentence.float().clone(), '../data/new_approach/train/sarcasm/sentence_layer.pt')\n",
    "torch.save(ground_truth.float().clone(), '../data/new_approach/train/sarcasm/y_train_sentence.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = []\n",
    "y_val = []\n",
    "batch_initial = torch.zeros((len(input_ids)-7000,4,1,768))\n",
    "batch_middle = torch.zeros((len(input_ids)-7000,4,1,768))\n",
    "batch_last = torch.zeros((len(input_ids)-7000,4,1,768))\n",
    "\n",
    "batch_initial_val = torch.zeros((7000,4,1,768))\n",
    "batch_middle_val = torch.zeros((7000,4,1,768))\n",
    "batch_last_val = torch.zeros((7000,4,1,768))\n",
    "index = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        \n",
    "        sentence_emb_1 = torch.mean(features[2][1], dim=1).view(1, -1) #layer 1 \n",
    "        sentence_emb_2 = torch.mean(features[2][2], dim=1).view(1, -1)\n",
    "        sentence_emb_3 = torch.mean(features[2][3], dim=1).view(1, -1)\n",
    "        sentence_emb_4 = torch.mean(features[2][4], dim=1).view(1, -1)\n",
    "        sentence_emb_5 = torch.mean(features[2][5], dim=1).view(1, -1)\n",
    "        sentence_emb_6 = torch.mean(features[2][6], dim=1).view(1, -1)\n",
    "        sentence_emb_7 = torch.mean(features[2][7], dim=1).view(1, -1)\n",
    "        sentence_emb_8 = torch.mean(features[2][8], dim=1).view(1, -1)\n",
    "        sentence_emb_9 = torch.mean(features[2][9], dim=1).view(1, -1)\n",
    "        sentence_emb_10 = torch.mean(features[2][10], dim=1).view(1, -1)\n",
    "        sentence_emb_11 = torch.mean(features[2][11], dim=1).view(1, -1)\n",
    "        sentence_emb_12 = torch.mean(features[2][12], dim=1).view(1, -1) #layer 12\n",
    "\n",
    "        sub_layers_initial = torch.stack((sentence_emb_1, sentence_emb_2, sentence_emb_3, sentence_emb_4), dim= 1).reshape(1,4,1,768)  #add batch dimension\n",
    "        sub_layers_middle = torch.stack((sentence_emb_5, sentence_emb_6, sentence_emb_7, sentence_emb_8), dim= 1).reshape(1,4,1,768)\n",
    "        sub_layers_last = torch.stack((sentence_emb_9, sentence_emb_10, sentence_emb_11, sentence_emb_12), dim= 1).reshape(1,4,1,768)\n",
    "        if i < 7000:\n",
    "            \n",
    "            batch_initial_val[i,:] = sub_layers_initial\n",
    "            batch_middle_val[i,:] = sub_layers_middle\n",
    "            batch_last_val[i,:] = sub_layers_last\n",
    "            y_val.append(train_txt.label.iloc[i])\n",
    "        else:\n",
    "            batch_initial[index,:] = sub_layers_initial\n",
    "            batch_middle[index,:] = sub_layers_middle\n",
    "            batch_last[index,:] = sub_layers_last\n",
    "        \n",
    "            y_target.append(df.label.iloc[i])\n",
    "            index += 1\n",
    "            \n",
    "ground_val = torch.tensor(y_val, dtype = torch.float)             \n",
    "ground_truth = torch.tensor(y_target, dtype = torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = []\n",
    "batch_initial = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_middle = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_last = torch.zeros((len(input_ids),4,1,768))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        \n",
    "        sentence_emb_1 = torch.mean(features[2][1], dim=1).view(1, -1) #layer 1 \n",
    "        sentence_emb_2 = torch.mean(features[2][2], dim=1).view(1, -1)\n",
    "        sentence_emb_3 = torch.mean(features[2][3], dim=1).view(1, -1)\n",
    "        sentence_emb_4 = torch.mean(features[2][4], dim=1).view(1, -1)\n",
    "        sentence_emb_5 = torch.mean(features[2][5], dim=1).view(1, -1)\n",
    "        sentence_emb_6 = torch.mean(features[2][6], dim=1).view(1, -1)\n",
    "        sentence_emb_7 = torch.mean(features[2][7], dim=1).view(1, -1)\n",
    "        sentence_emb_8 = torch.mean(features[2][8], dim=1).view(1, -1)\n",
    "        sentence_emb_9 = torch.mean(features[2][9], dim=1).view(1, -1)\n",
    "        sentence_emb_10 = torch.mean(features[2][10], dim=1).view(1, -1)\n",
    "        sentence_emb_11 = torch.mean(features[2][11], dim=1).view(1, -1)\n",
    "        sentence_emb_12 = torch.mean(features[2][12], dim=1).view(1, -1) #layer 12\n",
    "\n",
    "        sub_layers_initial = torch.stack((sentence_emb_1, sentence_emb_2, sentence_emb_3, sentence_emb_4), dim= 1).reshape(1,4,1,768)  #add batch dimension\n",
    "        sub_layers_middle = torch.stack((sentence_emb_5, sentence_emb_6, sentence_emb_7, sentence_emb_8), dim= 1).reshape(1,4,1,768)\n",
    "        sub_layers_last = torch.stack((sentence_emb_9, sentence_emb_10, sentence_emb_11, sentence_emb_12), dim= 1).reshape(1,4,1,768)\n",
    "              \n",
    "        batch_initial[i,:] = sub_layers_initial\n",
    "        batch_middle[i,:] = sub_layers_middle\n",
    "        batch_last[i,:] = sub_layers_last\n",
    "        \n",
    "        y_target.append(df.label.iloc[i])\n",
    "\n",
    "ground_truth = torch.tensor(y_target, dtype = torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(batch_initial.float().clone(), '../data/new_approach/train/sarcasm/init_layer.pt')\n",
    "torch.save(batch_middle.float().clone(), '../data/new_approach/train/sarcasm/middle_layer.pt')\n",
    "torch.save(batch_last.float().clone(), '../data/new_approach/train/sarcasm/last_layer.pt')\n",
    "torch.save(ground_truth.float().clone(), '../data/new_approach/train/sarcasm/y_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(batch_initial_val.float().clone(), '../data/new_approach/train/sarcasm_validation/init_layer.pt')\n",
    "torch.save(batch_middle_val.float().clone(), '../data/new_approach/train/sarcasm_validation/middle_layer.pt')\n",
    "torch.save(batch_last_val.float().clone(), '../data/new_approach/train/sarcasm_validation/last_layer.pt')\n",
    "torch.save(ground_val.float().clone(), '../data/new_approach/train/sarcasm_validation/y_train.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract pos and embeddings layers from bert-tweet (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riloff = pd.read_csv('../data/Riloff_twitter/riloff_sarc_train_test.csv')\n",
    "ghosh = pd.read_csv('../data/Ghosh_sarc_tweet/Test_v1.txt', sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghosh.rename({0: 'training', 1:'label', 2:'text'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt_file_riloff = FeaturesText.preprocessing_text(riloff,remove_hashtags=True, remove_mentions=True, lowercase=True, arktweet_pos=True)\n",
    "test_txt_file_ghosh = FeaturesText.preprocessing_text(ghosh,remove_hashtags=True, remove_mentions=True, lowercase=True, arktweet_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test_txt_ril = test_txt_file_riloff.get_clean_df()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test_txt_ghos = test_txt_file_ghosh.get_clean_df()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_pos = tokenizer.texts_to_sequences(test_txt_ril['pos'].astype(str))\n",
    "data_pos = pad_sequences(sequences_pos, maxlen=30, padding='post', truncating='post')\n",
    "pos_tensor = torch.unsqueeze(torch.tensor(data_pos, dtype=torch.float),1)\n",
    "torch.save(pos_tensor.float().clone(), '../data/new_approach/test/sarcasm/pos_tensor_riloff.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_pos = tokenizer.texts_to_sequences(test_txt_ghos['pos'].astype(str))\n",
    "data_pos = pad_sequences(sequences_pos, maxlen=30, padding='post', truncating='post')\n",
    "pos_tensor = torch.unsqueeze(torch.tensor(data_pos, dtype=torch.float),1)\n",
    "torch.save(pos_tensor.float().clone(), '../data/new_approach/test/sarcasm/pos_tensor_ghosh.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [torch.tensor([tokenizer_bert.encode(i)]) for i in test_txt_ril.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentence = torch.zeros((len(input_ids),1,768))\n",
    "y_target = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        batch_sentence[i, :] = features[1]\n",
    "        y_target.append(train_txt.label.iloc[i])\n",
    "        \n",
    "ground_truth = torch.tensor(y_target, dtype = torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(batch_sentence.float().clone(), '../data/new_approach/test/sarcasm/sentence_layer_riloff.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = []\n",
    "batch_initial = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_middle = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_last = torch.zeros((len(input_ids),4,1,768))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        \n",
    "        sentence_emb_1 = torch.mean(features[2][1], dim=1).view(1, -1) #layer 1 \n",
    "        sentence_emb_2 = torch.mean(features[2][2], dim=1).view(1, -1)\n",
    "        sentence_emb_3 = torch.mean(features[2][3], dim=1).view(1, -1)\n",
    "        sentence_emb_4 = torch.mean(features[2][4], dim=1).view(1, -1)\n",
    "        sentence_emb_5 = torch.mean(features[2][5], dim=1).view(1, -1)\n",
    "        sentence_emb_6 = torch.mean(features[2][6], dim=1).view(1, -1)\n",
    "        sentence_emb_7 = torch.mean(features[2][7], dim=1).view(1, -1)\n",
    "        sentence_emb_8 = torch.mean(features[2][8], dim=1).view(1, -1)\n",
    "        sentence_emb_9 = torch.mean(features[2][9], dim=1).view(1, -1)\n",
    "        sentence_emb_10 = torch.mean(features[2][10], dim=1).view(1, -1)\n",
    "        sentence_emb_11 = torch.mean(features[2][11], dim=1).view(1, -1)\n",
    "        sentence_emb_12 = torch.mean(features[2][12], dim=1).view(1, -1) #layer 12\n",
    "\n",
    "        sub_layers_initial = torch.stack((sentence_emb_1, sentence_emb_2, sentence_emb_3, sentence_emb_4), dim= 1).reshape(1,4,1,768)  #add batch dimension\n",
    "        sub_layers_middle = torch.stack((sentence_emb_5, sentence_emb_6, sentence_emb_7, sentence_emb_8), dim= 1).reshape(1,4,1,768)\n",
    "        sub_layers_last = torch.stack((sentence_emb_9, sentence_emb_10, sentence_emb_11, sentence_emb_12), dim= 1).reshape(1,4,1,768)\n",
    "              \n",
    "        batch_initial[i,:] = sub_layers_initial\n",
    "        batch_middle[i,:] = sub_layers_middle\n",
    "        batch_last[i,:] = sub_layers_last\n",
    "        \n",
    "        y_target.append(test_txt_ril.labels.iloc[i])\n",
    "\n",
    "ground_truth = torch.tensor(y_target, dtype = torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(batch_initial.float().clone(), '../data/new_approach/test/sarcasm/init_layer_riloff.pt')\n",
    "torch.save(batch_middle.float().clone(), '../data/new_approach/test/sarcasm/middle_layer_riloff.pt')\n",
    "torch.save(batch_last.float().clone(), '../data/new_approach/test/sarcasm/last_layer_riloff.pt')\n",
    "torch.save(ground_truth.float().clone(), '../data/new_approach/test/sarcasm/y_riloff.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [torch.tensor([tokenizer_bert.encode(i, truncation=True, max_length=128)]) for i in test_txt_ghos.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentence = torch.zeros((len(input_ids),1,768))\n",
    "y_target = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        batch_sentence[i, :] = features[1]\n",
    "        y_target.append(train_txt.label.iloc[i])\n",
    "        \n",
    "ground_truth = torch.tensor(y_target, dtype = torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(batch_sentence.float().clone(), '../data/new_approach/test/sarcasm/sentence_layer_ghosh.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = []\n",
    "batch_initial = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_middle = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_last = torch.zeros((len(input_ids),4,1,768))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        \n",
    "        sentence_emb_1 = torch.mean(features[2][1], dim=1).view(1, -1) #layer 1 \n",
    "        sentence_emb_2 = torch.mean(features[2][2], dim=1).view(1, -1)\n",
    "        sentence_emb_3 = torch.mean(features[2][3], dim=1).view(1, -1)\n",
    "        sentence_emb_4 = torch.mean(features[2][4], dim=1).view(1, -1)\n",
    "        sentence_emb_5 = torch.mean(features[2][5], dim=1).view(1, -1)\n",
    "        sentence_emb_6 = torch.mean(features[2][6], dim=1).view(1, -1)\n",
    "        sentence_emb_7 = torch.mean(features[2][7], dim=1).view(1, -1)\n",
    "        sentence_emb_8 = torch.mean(features[2][8], dim=1).view(1, -1)\n",
    "        sentence_emb_9 = torch.mean(features[2][9], dim=1).view(1, -1)\n",
    "        sentence_emb_10 = torch.mean(features[2][10], dim=1).view(1, -1)\n",
    "        sentence_emb_11 = torch.mean(features[2][11], dim=1).view(1, -1)\n",
    "        sentence_emb_12 = torch.mean(features[2][12], dim=1).view(1, -1) #layer 12\n",
    "\n",
    "        sub_layers_initial = torch.stack((sentence_emb_1, sentence_emb_2, sentence_emb_3, sentence_emb_4), dim= 1).reshape(1,4,1,768)  #add batch dimension\n",
    "        sub_layers_middle = torch.stack((sentence_emb_5, sentence_emb_6, sentence_emb_7, sentence_emb_8), dim= 1).reshape(1,4,1,768)\n",
    "        sub_layers_last = torch.stack((sentence_emb_9, sentence_emb_10, sentence_emb_11, sentence_emb_12), dim= 1).reshape(1,4,1,768)\n",
    "              \n",
    "        batch_initial[i,:] = sub_layers_initial\n",
    "        batch_middle[i,:] = sub_layers_middle\n",
    "        batch_last[i,:] = sub_layers_last\n",
    "        \n",
    "        y_target.append(test_txt_ghos.label.iloc[i])\n",
    "\n",
    "ground_truth = torch.tensor(y_target, dtype = torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(batch_initial.float().clone(), '../data/new_approach/test/sarcasm/init_layer_ghosh.pt')\n",
    "torch.save(batch_middle.float().clone(), '../data/new_approach/test/sarcasm/middle_layer_ghosh.pt')\n",
    "torch.save(batch_last.float().clone(), '../data/new_approach/test/sarcasm/last_layer_ghosh.pt')\n",
    "torch.save(ground_truth.float().clone(), '../data/new_approach/test/sarcasm/y_ghosh.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
