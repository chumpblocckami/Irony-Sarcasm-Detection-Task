{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer \n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from torch import nn\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from Attention_Augmented_Conv2d.attention_augmented_conv import AugmentedConv\n",
    "use_cuda = torch.cuda.is_available()\n",
    "from sklearn.metrics import f1_score\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\",output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",  add_special_tokens=True,\n",
    "                                                max_length=20, pad_to_max_length=True,normalization=True, truncation=True, padding= True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'D:/Data_Science_all/MSC_2_anno/Tesi_Irony_Sarcasm/data/final_sarc_trainingset_twitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 1) \n",
    "data['text'] = data['text'].str.replace(r'#([^\\s:]+)', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony = pd.read_csv('D:/Data_Science_all/MSC_2_anno/Tesi_Irony_Sarcasm/data/final_training_irony.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with one sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [i.split(' ') for i in data.text]\n",
    "text_irony = [i.split(' ') for i in irony.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [len(i) for i in text]\n",
    "count_irony =  [len(i) for i in text_irony]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([count, count_irony]).T.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(count).quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "sns.histplot(np.array(count), color='red')\n",
    "sns.histplot(np.array(count_irony))\n",
    "plt.xlim(0,60)\n",
    "plt.legend(['sarcasm', 'irony'], prop={\"size\":15})\n",
    "plt.axvline(20, 0,linestyle='--', color = 'blue')\n",
    "plt.text(19,3750,'Cut-off length, third quantile',rotation=90, fontsize = 15)\n",
    "plt.rc('xtick',labelsize=15)\n",
    "plt.rc('ytick',labelsize=15)\n",
    "plt.xlabel('Length tweets', fontsize = 20)\n",
    "plt.ylabel('Count', fontsize = 20)\n",
    "plt.title('Distribution tweets length, divided by sarcasm and irony', fontsize = 25)\n",
    "plt.savefig('D:/Data_Science_all/MSC_2_anno/Tesi_Irony_Sarcasm/Code/Plots/distribution_word_length.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(data.text[58445], max_length=25, truncation=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = bertweet(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each tensor is n_batches x n_tokens x 768 features f\n",
    "encoder_embedding = features[2][0] #word embeddings\n",
    "encoder_1 = features[2][1] \n",
    "encoder_2 = features[2][2]\n",
    "encoder_3 = features[2][3]\n",
    "encoder_4 = features[2][4]\n",
    "encoder_5 = features[2][5]\n",
    "encoder_6 = features[2][6]\n",
    "encoder_7 = features[2][7]\n",
    "encoder_8 = features[2][8]\n",
    "encoder_9 = features[2][9]\n",
    "encoder_10 = features[2][10]\n",
    "encoder_11 = features[2][11]\n",
    "encoder_12 = features[2][12] #last hidden encoder layer output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[2][1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(tensor):\n",
    "    length = tensor.size(1)\n",
    "    if length >= 20:\n",
    "        max_tensor = torch.squeeze(tensor).T[:,:20]\n",
    "    else:\n",
    "        max_tensor = F.pad(torch.squeeze(tensor).T, pad=(0, abs(torch.squeeze(tensor).T.size(1) - 20)), mode='constant', value=0)\n",
    "    return torch.unsqueeze(max_tensor.T,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_layers_initial = torch.stack((sentence_emb_1, sentence_emb_2, sentence_emb_3, sentence_emb_4), dim= 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate the vectors from the last four layers. \n",
    "It seems that the last 4 hidden output layers, if the concatenation between them is applied, achieve higher accuracy with respect to the other layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "cat_vec = torch.cat((encoder_9[0],encoder_10[0], encoder_11[0], encoder_12[0]), dim=1)    \n",
    "cat_vec_first = torch.cat((encoder_1[0],encoder_2[0], encoder_3[0], encoder_4[0]), dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos(torch.mean(cat_vec_first, dim = 0).view(1,-1), torch.mean(cat_vec, dim = 0).view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_last_four = encoder_12[0] + encoder_11[0] + encoder_10[0] + encoder_9[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_first_four =  encoder_1[0] + encoder_2[0] + encoder_3[0] + encoder_4[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_middle_four = encoder_5[0] + encoder_6[0] + encoder_7[0] + encoder_8[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_last_four = torch.mean(sum_last_four, dim = 0).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How sentence embeddings are created: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average of all n tokens of the lastr hidden layer\n",
    "sentence_embedding = torch.mean(encoder_12[0], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generate 12 tensor for the sentence embedding, each for encoder layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vec = torch.cat((sentence_emb_9,sentence_emb_10, sentence_emb_11, sentence_emb_12), dim=1)    \n",
    "cat_vec_first = torch.cat((sentence_emb_1 ,sentence_emb_2, sentence_emb_3, sentence_emb_4), dim=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all features from the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract each sentence embedding vector for each hidden state layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [torch.tensor([tokenizer.encode(i, truncation=True, max_length=70)]) for i in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        \n",
    "        sentence_emb_1 = torch.mean(features[2][1], dim=1).view(1, -1) #layer 1 \n",
    "        sentence_emb_2 = torch.mean(features[2][2], dim=1).view(1, -1)\n",
    "        sentence_emb_3 = torch.mean(features[2][3], dim=1).view(1, -1)\n",
    "        sentence_emb_4 = torch.mean(features[2][4], dim=1).view(1, -1)\n",
    "        sentence_emb_5 = torch.mean(features[2][5], dim=1).view(1, -1)\n",
    "        sentence_emb_6 = torch.mean(features[2][6], dim=1).view(1, -1)\n",
    "        sentence_emb_7 = torch.mean(features[2][7], dim=1).view(1, -1)\n",
    "        sentence_emb_8 = torch.mean(features[2][8], dim=1).view(1, -1)\n",
    "        sentence_emb_9 = torch.mean(features[2][9], dim=1).view(1, -1)\n",
    "        sentence_emb_10 = torch.mean(features[2][10], dim=1).view(1, -1)\n",
    "        sentence_emb_11 = torch.mean(features[2][11], dim=1).view(1, -1)\n",
    "        sentence_emb_12 = torch.mean(features[2][12], dim=1).view(1, -1) #layer 12\n",
    "        # B x C x H x W, 1 x 4 x 1 x 768\n",
    "        sub_layers_initial = torch.stack((sentence_emb_1, sentence_emb_2, sentence_emb_3, sentence_emb_4), dim= 1).reshape(1,4,1,768)  #add batch dimension\n",
    "        sub_layers_middle = torch.stack((sentence_emb_5, sentence_emb_6, sentence_emb_7, sentence_emb_8), dim= 1).reshape(1,4,1,768)\n",
    "        sub_layers_last = torch.stack((sentence_emb_9, sentence_emb_10, sentence_emb_11, sentence_emb_12), dim= 1).reshape(1,4,1,768)\n",
    "        \n",
    "        torch.save(sub_layers_initial.float().clone(), '../data/new_approach/train/sarcasm/id_{}_init_{}.pt'.format(i, data.label.iloc[i]))\n",
    "        torch.save(sub_layers_middle.float().clone(), '../data/new_approach/train/sarcasm/id_{}_middle_{}.pt'.format(i, data.label.iloc[i]))\n",
    "        torch.save(sub_layers_last.float().clone(), '../data/new_approach/train/sarcasm/id_{}_last_{}.pt'.format(i, data.label.iloc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = []\n",
    "#batch_last = torch.zeros((len(input_ids),4,768,50))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) \n",
    "        sentence_emb_9 = zero_padding(features[2][9])\n",
    "        sentence_emb_10 = zero_padding(features[2][10])\n",
    "        sentence_emb_11 = zero_padding(features[2][11])\n",
    "        sentence_emb_12 = zero_padding(features[2][12]) \n",
    "        \n",
    "        sub_layers_last = torch.stack((sentence_emb_9, sentence_emb_10, sentence_emb_11, sentence_emb_12), dim= 1)\n",
    "        if i <= 15000:\n",
    "            torch.save(sub_layers_last.float().clone(), '../data/new_approach/validation/sarcasm_word/features/{}_.pt'.format(i))\n",
    "            torch.save(torch.tensor(data.label.iloc[i], dtype = torch.float), '../data/new_approach/validation/sarcasm_word/labels/{}_.pt'.format(i))\n",
    "        else:\n",
    "            torch.save(sub_layers_last.float().clone(), '../data/new_approach/train/sarcasm_word/features/{}_.pt'.format(i))\n",
    "            torch.save(torch.tensor(data.label.iloc[i], dtype = torch.float), '../data/new_approach/train/sarcasm_word/labels/{}_.pt'.format(i)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simple_attention, self).__init__()\n",
    "        \n",
    "        self.conv_att = AugmentedConv(in_channels=4, out_channels=256, kernel_size=2, dk=3, dv=3, Nh=3, relative=False, stride=2)\n",
    "        self.pooling1 = nn.AvgPool3d(kernel_size=(1,1,1), stride = (2,1,2))\n",
    "        self.conv1 = nn.Conv2d(in_channels = 128, out_channels=64, kernel_size=1, stride = 1)\n",
    "        self.pooling2 =  nn.AvgPool3d(kernel_size=(1,1,2), stride = (2,1,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels=16, kernel_size=1, stride = 2)\n",
    "        self.max_pool = nn.MaxPool3d(kernel_size=(1,1,3), stride = (1,1,3))\n",
    "        self.bgru = nn.GRU(input_size=3328, hidden_size=64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.fc3 = nn.Linear(32,16)\n",
    "        self.fc4 = nn.Linear(16,1)\n",
    "    \n",
    "    def forward(self, input1):\n",
    "        conv_atten = self.conv_att(input1)\n",
    "        conv_pooled1 = self.pooling1(conv_atten)\n",
    "        conv_simple = self.conv1(conv_pooled1)\n",
    "        conv_pooled2 = self.pooling2(conv_simple)\n",
    "        conv_simple = self.conv2(conv_pooled2)\n",
    "        conv_max = self.max_pool(conv_simple)\n",
    "        flatten = torch.flatten(conv_max).reshape(conv_max.size(0), 1, 3328)\n",
    "        gru = self.bgru(flatten)\n",
    "        dense1 = F.relu(self.fc1(gru))\n",
    "        dense2 = F.relu(self.fc2(dense1))\n",
    "        dense3 = F.relu(self.fc3(dense2))\n",
    "        output = self.fc4(dense3)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvolutionalAttention_boosted(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         super(ConvolutionalAttention_boosted, self).__init__()\n",
    "        \n",
    "#         self.conv_att = AugmentedConv(in_channels=4, out_channels=256, kernel_size=3, dk=84, dv=12, Nh=12, relative=False, stride=1)\n",
    "#         self.pooling1 = nn.AvgPool3d(kernel_size=(1,1,1), stride = (2,1,2))\n",
    "#         self.drop4 = nn.Dropout(0.2)\n",
    "#         self.conv1 = nn.Conv2d(in_channels = 128, out_channels=128, kernel_size=1, stride = 2)\n",
    "#         self.drop3 = nn.Dropout(0.3)\n",
    "#         self.pooling2 = nn.AvgPool3d(kernel_size=(1,1,2), stride = (1,1,2))\n",
    "#         self.conv2 =  nn.Conv2d(in_channels = 128, out_channels=128, kernel_size=1, stride = 2)\n",
    "#         self.max = nn.MaxPool3d(kernel_size=(1,1,2), stride = (1,1,2))\n",
    "#         self.conv3 = nn.Conv2d(in_channels = 128, out_channels=64, kernel_size=1, stride = 2)\n",
    "#         self.max2 = nn.MaxPool3d(kernel_size=(64,1,1), stride = (16,1,1))\n",
    "#         self.bgru = nn.GRU(input_size=24, hidden_size=32, num_layers=1, batch_first=True, bidirectional=True)\n",
    "#         self.drop2 = nn.Dropout(0.3)\n",
    "#         self.fc1 = nn.Linear(192, 32)\n",
    "#         self.drop1 =  nn.Dropout(0.5)\n",
    "#         self.fc_out = nn.Linear(32, 1)\n",
    "        \n",
    "#     def forward(self, input1, input2, input3):\n",
    "        \n",
    "#         sub_layer_conv = self.conv_att(input1)\n",
    "#         midd_layer_conv = self.conv_att(input2)\n",
    "#         high_layer_conv = self.conv_att(input3)\n",
    "\n",
    "#         sub_layer_pool = self.pooling1(sub_layer_conv)\n",
    "#         midd_layer_pool = self.pooling1(midd_layer_conv)\n",
    "#         high_layer_pool = self.pooling1(high_layer_conv)\n",
    "        \n",
    "#         drop_pool_sub = self.drop4(sub_layer_pool)\n",
    "#         drop_pool_mid = self.drop4(sub_layer_pool)\n",
    "#         drop_pool_last = self.drop4(sub_layer_pool)\n",
    "        \n",
    "#         sub_layer_conv = self.conv1(drop_pool_sub)\n",
    "#         midd_layer_conv = self.conv1(drop_pool_mid)\n",
    "#         high_layer_conv = self.conv1(drop_pool_last)\n",
    "        \n",
    "#         drop_sub = self.drop3(sub_layer_conv)\n",
    "#         drop_mid = self.drop3(midd_layer_conv)\n",
    "#         drop_las = self.drop3(high_layer_conv)\n",
    "        \n",
    "#         sub_layer = self.pooling2(drop_sub)\n",
    "#         midd_layer = self.pooling2(drop_mid)\n",
    "#         high_layer = self.pooling2(drop_las)\n",
    "        \n",
    "                               \n",
    "#         sub_conv2 = self.conv2(sub_layer)\n",
    "#         mid_conv2 = self.conv2(midd_layer)\n",
    "#         high_conv2 = self.conv2(high_layer)\n",
    "                               \n",
    "                               \n",
    "#         sub_max = self.max(sub_conv2)\n",
    "#         mid_max = self.max(mid_conv2)\n",
    "#         high_max = self.max(high_conv2)\n",
    "                               \n",
    "#         sub_conv3 = self.conv3(sub_max)\n",
    "#         mid_conv3 = self.conv3(mid_max)\n",
    "#         hig_conv3 = self.conv3(high_max)\n",
    "        \n",
    "#         sub_max2 = self.max2(sub_conv3)\n",
    "#         mid_max2 = self.max2(mid_conv3)\n",
    "#         high_max2 = self.max2(hig_conv3)\n",
    "\n",
    "#         gru1_out, gru1_hidden = self.bgru(sub_max2.reshape(sub_max2.size(0),1,sub_max2.size(-1)))\n",
    "#         gru2_out, gru2_hidden = self.bgru(mid_max2.reshape(mid_max2.size(0),1,mid_max2.size(-1)))\n",
    "#         gru3_out, gru3_hidden = self.bgru(high_max2.reshape(high_max2.size(0),1,high_max2.size(-1)))\n",
    "        \n",
    "#         combined = torch.cat((gru1_out, gru2_out, gru3_out), dim=2)\n",
    "        \n",
    "#         drop_comb = self.drop2(combined)\n",
    "#         dense1 = F.relu(self.fc1(drop_comb))\n",
    "#         drop = self.drop1(dense1)\n",
    "#         dense2 = self.fc_out(drop)\n",
    "        \n",
    "#         return dense2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ConvolutionalAttention, self).__init__()\n",
    "        \n",
    "        self.conv_att = AugmentedConv(in_channels=4, out_channels=32, kernel_size=3, dk=42, dv=6, Nh=6, relative=False, stride=1)\n",
    "        self.pooling1 = nn.AvgPool3d(kernel_size=(1,1,1), stride = (2,1,1))\n",
    "        self.drop4 = nn.Dropout(0.2)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 128, out_channels=128, kernel_size=1, stride = 2)\n",
    "        self.drop3 = nn.Dropout(0.3)\n",
    "        self.pooling2 = nn.AvgPool3d(kernel_size=(1,1,1), stride = (2,1,1))\n",
    "        self.conv2 =  nn.Conv2d(in_channels = 64, out_channels=128, kernel_size=1, stride = 2)\n",
    "        self.bgru = nn.GRU(input_size=768, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.drop1 =  nn.Dropout(0.5)\n",
    "        self.fc_out = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, input1, input2, input3):\n",
    "        \n",
    "        sub_layer_conv = self.conv_att(input1)\n",
    "        midd_layer_conv = self.conv_att(input2)\n",
    "        high_layer_conv = self.conv_att(input3)\n",
    "\n",
    "        sub_layer_pool = self.pooling1(sub_layer_conv)\n",
    "        midd_layer_pool = self.pooling1(midd_layer_conv)\n",
    "        high_layer_pool = self.pooling1(high_layer_conv)\n",
    "        \n",
    "        drop_pool_sub = self.drop4(sub_layer_pool)\n",
    "        drop_pool_mid = self.drop4(sub_layer_pool)\n",
    "        drop_pool_last = self.drop4(sub_layer_pool)\n",
    "        \n",
    "        sub_layer_conv = self.conv1(drop_pool_sub)\n",
    "        midd_layer_conv = self.conv1(drop_pool_mid)\n",
    "        high_layer_conv = self.conv1(drop_pool_last)\n",
    "        \n",
    "        drop_sub = self.drop3(sub_layer_conv)\n",
    "        drop_mid = self.drop3(midd_layer_conv)\n",
    "        drop_las = self.drop3(high_layer_conv)\n",
    "        \n",
    "        sub_layer = self.pooling2(drop_sub)\n",
    "        midd_layer = self.pooling2(drop_mid)\n",
    "        high_layer = self.pooling2(drop_las)\n",
    "        \n",
    "        gru1_out, gru1_hidden = self.bgru(sub_layer.reshape(sub_layer.size(0),1,768))\n",
    "        gru2_out, gru2_hidden = self.bgru(midd_layer.reshape(midd_layer.size(0),1,768))\n",
    "        gru3_out, gru3_hidden = self.bgru(high_layer.reshape(high_layer.size(0),1,768))\n",
    "        \n",
    "        combined = torch.cat((gru1_out, gru2_out, gru3_out), dim=2)\n",
    "        \n",
    "        drop_comb = self.drop2(combined)\n",
    "        dense1 = F.relu(self.fc1(drop_comb))\n",
    "        drop = self.drop1(dense1)\n",
    "        dense2 = self.fc_out(drop)\n",
    "        \n",
    "        return dense2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvolutionalAttention_light(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         super(ConvolutionalAttention_light, self).__init__()\n",
    "        \n",
    "#         self.conv_att = AugmentedConv(in_channels=4, out_channels=256, kernel_size=3, dk=42, dv=6, Nh=6, relative=False, stride=1)\n",
    "#         self.pooling1 = nn.AvgPool3d(kernel_size=(1,1,1), stride = (2,1,1))\n",
    "#         #self.drop4 = nn.Dropout(0.2)\n",
    "#         self.conv1 = nn.Conv2d(in_channels = 128, out_channels=128, kernel_size=1, stride = 2)\n",
    "#         self.drop3 = nn.Dropout(0.4)\n",
    "#         self.pooling2 = nn.AvgPool3d(kernel_size=(1,1,1), stride = (2,1,1))\n",
    "#         self.conv_att2 = AugmentedConv(in_channels=64, out_channels=64, kernel_size=3, dk=42, dv=6, Nh=6, relative=False, stride=1)\n",
    "#         self.max = nn.MaxPool3d(kernel_size=(1,1,2), stride = (64,1,4))\n",
    "#         self.bgru = nn.GRU(input_size=96, hidden_size=64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "#         self.drop2 = nn.Dropout(0.4)\n",
    "#         self.fc1 = nn.Linear(128, 64)\n",
    "#         #self.fc2 = nn.Linear(128, 64)\n",
    "#         #self.fc3 = nn.Linear(64,32)\n",
    "#         self.drop1 =  nn.Dropout(0.5)\n",
    "#         self.fc_out = nn.Linear(64, 1)\n",
    "        \n",
    "#     def forward(self, input1):\n",
    "        \n",
    "#         sub_layer_conv = self.conv_att(input1)\n",
    "\n",
    "#         sub_layer_pool = self.pooling1(sub_layer_conv)\n",
    "        \n",
    "#         drop_pool_sub = self.drop3(sub_layer_pool) \n",
    "        \n",
    "#         sub_layer_conv = self.conv1(sub_layer_pool)\n",
    "        \n",
    "#         drop_sub = self.drop3(sub_layer_conv)\n",
    "        \n",
    "#         sub_layer = self.pooling2(drop_sub)\n",
    "        \n",
    "#         sub_layer_att = self.conv_att2(sub_layer)\n",
    "        \n",
    "#         max_pool = self.max(sub_layer_att)\n",
    "\n",
    "#         gru1_out, gru1_hidden = self.bgru(max_pool.reshape(max_pool.size(0),1,96))\n",
    "        \n",
    "#         drop_comb = self.drop2(gru1_out)\n",
    "#         dense1 = F.relu(self.fc1(drop_comb))\n",
    "#         drop = self.drop1(dense1)\n",
    "#         #layer = F.relu(self.fc2(drop))\n",
    "#         #drop_f = self.drop1(layer)\n",
    "#         #layer2 = F.relu(self.fc3(drop))\n",
    "#         #drop_f2 = self.drop1(layer2)\n",
    "#         dense2 = self.fc_out(drop)\n",
    "        \n",
    "#         return dense2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = simple_attention()\n",
    "mymodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in mymodel.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.abspath('.')) \n",
    "data_dir = ROOT_DIR + '\\\\{}\\\\{}\\\\{}\\\\{}\\\\'.format('data', 'new_approach', 'train', 'sarcasm_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = data_dir + 'features\\\\'\n",
    "label_dir = data_dir + 'labels\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_number(elem):\n",
    "    return int(re.findall('(\\d*)(_.pt)', elem)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_feat, root_label):\n",
    "        self.files = os.listdir(root_feat)\n",
    "        self.labels = os.listdir(root_label)\n",
    "        self.files.sort(reverse=False, key=sort_number)\n",
    "        self.labels.sort(reverse=False, key=sort_number)\n",
    "        \n",
    "        self.root_feat = root_feat\n",
    "        self.root_label = root_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.load(os.path.join(self.root_feat, self.files[idx])) # load the features of this sample\n",
    "        label = torch.load(os.path.join(self.root_label, self.labels[idx]))\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(features_dir,label_dir)\n",
    "trainloader = torch.utils.data.DataLoader(dataset,shuffle=True,batch_size=1,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_file(data_dir, idx):\n",
    "#     for i in os.listdir(data_dir):\n",
    "#         if i.startswith('id_{}_init_'.format(idx)):\n",
    "#             tensor_init = torch.load(data_dir+i)\n",
    "#             y = int(re.findall('(\\w{4,10})_(\\d)', i)[0][1])\n",
    "#         if i.startswith('id_{}_middle_'.format(idx)):\n",
    "#             tensor_middle = torch.load(data_dir+i)\n",
    "#         if i.startswith('id_{}_last_'.format(idx)):\n",
    "#             tensor_last = torch.load(data_dir+i)\n",
    "#     yield tensor_init, tensor_middle, tensor_last, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Dataloader(data_dir, batch_size):\n",
    "#     folder = os.listdir(data_dir)\n",
    "#     n_batches_per_epoch = int(len(folder)/3)//batch_size\n",
    "#     for i in range(n_batches_per_epoch):\n",
    "#         idx = list(range(int(len(folder)/3))[batch_size*i:batch_size*(i+1)])\n",
    "#         idx_batch = range(batch_size)\n",
    "#         batch_initial = torch.zeros((batch_size,4,1,768))\n",
    "#         batch_middle = torch.zeros((batch_size,4,1,768))\n",
    "#         batch_last = torch.zeros((batch_size,4,1,768))\n",
    "#         y_target = []\n",
    "#         for j in idx_batch:\n",
    "#             try:\n",
    "#                 tensor_init, tensor_middle, tensor_last, y = next(find_file(data_dir, idx[j]))\n",
    "#                 y_target.append(y)\n",
    "#                 batch_initial[j,:] = tensor_init\n",
    "#                 batch_middle[j,:] = tensor_middle\n",
    "#                 batch_last[j,:] = tensor_last\n",
    "#             except StopIteration:\n",
    "#                 batch_initial = batch_initial[:j,:]\n",
    "#                 batch_middle = batch_middle[:j,:]\n",
    "#                 batch_last = batch_last[:j,:]\n",
    "#                 break\n",
    "#         ground_truth = torch.tensor(y_target, dtype = torch.float)\n",
    "#         y_target = []\n",
    "#         yield batch_initial, batch_middle, batch_last, torch.unsqueeze(ground_truth,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data loader with tenor on ram \n",
    "# def ramloader(batch_size, ground_truth, batch_initial,batch_middle,batch_last):\n",
    "#     n_batches_per_epoch = ground_truth.shape[0]//batch_size\n",
    "#     for i in range(n_batches_per_epoch):\n",
    "#         idx = list(range(ground_truth.shape[0])[batch_size*i:batch_size*(i+1)])\n",
    "#         try:\n",
    "#             y_target = ground_truth[idx]\n",
    "#             batch_init = batch_initial[idx,:]\n",
    "#             batch_mid = batch_middle[idx,:]\n",
    "#             batch_la = batch_last[idx, :]\n",
    "#         except StopIteration:\n",
    "#             batch_init = batch_initial[:idx[-1]+1,:]\n",
    "#             batch_mid = batch_middle[:idx[-1]+1,:]\n",
    "#             batch_la = batch_last[:idx[-1]+1,:]\n",
    "#             break    \n",
    "#         yield batch_init, batch_mid, batch_la, torch.unsqueeze(y_target,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ramloader_light(batch_size, ground_truth,batch_middle):\n",
    "#     n_batches_per_epoch = ground_truth.shape[0]//batch_size\n",
    "#     for i in range(n_batches_per_epoch):\n",
    "#         idx = list(range(ground_truth.shape[0])[batch_size*i:batch_size*(i+1)])\n",
    "#         try:\n",
    "#             y_target = ground_truth[idx]\n",
    "#             batch_mid = batch_middle[idx,:]\n",
    "#         except StopIteration:\n",
    "#             batch_mid = batch_middle[:idx[-1]+1,:]\n",
    "#             break    \n",
    "#         yield batch_mid, torch.unsqueeze(y_target,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time \n",
    "# start_time = time.time()\n",
    "# batch_initial, batch_middle, batch_last, y = next(Dataloader(data_dir, 16))\n",
    "# print(\"--- Batch 16: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# batch_initial, batch_middle, batch_last, y = next(Dataloader(data_dir, 32))\n",
    "# print(\"--- Batch 32: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# batch_initial, batch_middle, batch_last, y = next(Dataloader(data_dir, 64))\n",
    "# print(\"--- Batch 64: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all on ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = []\n",
    "batch_initial = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_middle = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_last = torch.zeros((len(input_ids),4,1,768))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        \n",
    "        sentence_emb_1 = torch.mean(features[2][1], dim=1).view(1, -1) #layer 1 \n",
    "        sentence_emb_2 = torch.mean(features[2][2], dim=1).view(1, -1)\n",
    "        sentence_emb_3 = torch.mean(features[2][3], dim=1).view(1, -1)\n",
    "        sentence_emb_4 = torch.mean(features[2][4], dim=1).view(1, -1)\n",
    "        sentence_emb_5 = torch.mean(features[2][5], dim=1).view(1, -1)\n",
    "        sentence_emb_6 = torch.mean(features[2][6], dim=1).view(1, -1)\n",
    "        sentence_emb_7 = torch.mean(features[2][7], dim=1).view(1, -1)\n",
    "        sentence_emb_8 = torch.mean(features[2][8], dim=1).view(1, -1)\n",
    "        sentence_emb_9 = torch.mean(features[2][9], dim=1).view(1, -1)\n",
    "        sentence_emb_10 = torch.mean(features[2][10], dim=1).view(1, -1)\n",
    "        sentence_emb_11 = torch.mean(features[2][11], dim=1).view(1, -1)\n",
    "        sentence_emb_12 = torch.mean(features[2][12], dim=1).view(1, -1) #layer 12\n",
    "        # B x C x H x W, 1 x 4 x 1 x 768\n",
    "        sub_layers_initial = torch.stack((sentence_emb_1, sentence_emb_2, sentence_emb_3, sentence_emb_4), dim= 1).reshape(1,4,1,768)  #add batch dimension\n",
    "        sub_layers_middle = torch.stack((sentence_emb_5, sentence_emb_6, sentence_emb_7, sentence_emb_8), dim= 1).reshape(1,4,1,768)\n",
    "        sub_layers_last = torch.stack((sentence_emb_9, sentence_emb_10, sentence_emb_11, sentence_emb_12), dim= 1).reshape(1,4,1,768)\n",
    "              \n",
    "        batch_initial[i,:] = sub_layers_initial\n",
    "        batch_middle[i,:] = sub_layers_middle\n",
    "        batch_last[i,:] = sub_layers_last\n",
    "        \n",
    "        y_target.append(data.label.iloc[i])\n",
    "\n",
    "ground_truth = torch.tensor(y_target, dtype = torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(batch_initial.float().clone(), '../data/new_approach/train/sarcasm/init_layer.pt')\n",
    "# torch.save(batch_middle.float().clone(), '../data/new_approach/train/sarcasm/middle_layer.pt')\n",
    "# torch.save(batch_last.float().clone(), '../data/new_approach/train/sarcasm/last_layer.pt')\n",
    "# torch.save(ground_truth.float().clone(), '../data/new_approach/train/sarcasm/y_train.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_initial = torch.load('../data/new_approach/train/sarcasm/init_layer.pt')\n",
    "batch_middle = torch.load( '../data/new_approach/train/sarcasm/middle_layer.pt')\n",
    "batch_last = torch.load('../data/new_approach/train/sarcasm/last_layer.pt')\n",
    "ground_truth = torch.load('../data/new_approach/train/sarcasm/y_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_initial_train =  batch_initial[15713:]\n",
    "batch_middle_train = batch_middle[15713:]\n",
    "batch_last_train = batch_last[15713:]\n",
    "ground_truth_train = ground_truth[15713:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_initial_val = batch_initial[:15713]\n",
    "batch_middle_val = batch_middle[:15713]\n",
    "batch_last_val = batch_last[:15713]\n",
    "ground_truth_val = ground_truth[:15713]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(mymodel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def get_accuracy(output, actual):\n",
    "    \"\"\"\n",
    "    Return the accuracy of the model on the input data and actual ground truth.\n",
    "    \"\"\"\n",
    "    prob = torch.sigmoid(output)\n",
    "    pred = torch.squeeze((prob > 0.50).type(torch.FloatTensor))\n",
    "    accuracy = accuracy_score(pred.cpu(), torch.squeeze(actual).cpu())\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pred(pred):\n",
    "    numpy_list = [i.numpy() for i in pred]\n",
    "    numpy_1vec = np.concatenate(numpy_list).ravel()\n",
    "    return numpy_1vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All layers featuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.train()\n",
    "accuracy_epoch = []\n",
    "loss_epoch = []\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    trainloader = ramloader(4, ground_truth_train, batch_initial_train,batch_middle_train,batch_last_train)\n",
    "    accuracy_step = []\n",
    "    loss_step = []\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        layer_init = data[0].to(device)\n",
    "        layer_middle = data[1].to(device)\n",
    "        layer_high = data[2].to(device)\n",
    "        labels = data[3].to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = mymodel(layer_init, layer_middle, layer_high)\n",
    "        loss = criterion(outputs, torch.unsqueeze(labels, -1))\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        stepsize = int(ground_truth_train.shape[0]//4)\n",
    "        accuracy = get_accuracy(outputs, labels)\n",
    "        accuracy_step.append(accuracy)\n",
    "        loss_step.append(loss)\n",
    "        print('Epoch {}, Step {}/{}, Loss: {}, Accuracy: {}'.format(epoch,i,stepsize, loss, accuracy), end = '\\r')\n",
    "    mean_accuracy = np.mean(accuracy_step)\n",
    "    accuracy_epoch.append(mean_accuracy)\n",
    "    loss_epoch.append(loss_step)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_val = []\n",
    "    valoader = ramloader(16, ground_truth_val, batch_initial_val, batch_middle_val, batch_last_val)\n",
    "    accuracy_step = []\n",
    "    loss_step = []\n",
    "    for i, data in enumerate(valoader):\n",
    "\n",
    "        layer_init = data[0].to(device)\n",
    "        layer_mid = data[1].to(device)\n",
    "        layer_last = data[2].to(device)\n",
    "        labels = data[3].to(device)\n",
    "\n",
    "        outputs = mymodel(layer_init, layer_mid, layer_last)\n",
    "        accuracy = get_accuracy(outputs, labels)\n",
    "        prob = torch.sigmoid(outputs)\n",
    "        pred = torch.squeeze((prob > 0.50).type(torch.FloatTensor))\n",
    "        prediction_val.append(pred.cpu())\n",
    "        accuracy_step.append(accuracy)\n",
    "    mean_accuracy = np.mean(accuracy_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(normalize_pred(prediction_val), ground_truth_val[:15712]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.train()\n",
    "accuracy_epoch = []\n",
    "loss_epoch = []\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    trainloader = ramloader_light(32, ground_truth_train, batch_last_train)\n",
    "    accuracy_step = []\n",
    "    loss_step = []\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        layer_init = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = mymodel(layer_init)\n",
    "        loss = criterion(outputs, torch.unsqueeze(labels, -1))\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        stepsize = int(ground_truth.shape[0]//32)\n",
    "        accuracy = get_accuracy(outputs, labels)\n",
    "        accuracy_step.append(accuracy)\n",
    "        loss_step.append(loss)\n",
    "        print('Epoch {}, Step {}/{}, Loss: {}, Accuracy: {}'.format(epoch,i,stepsize, loss, accuracy), end = '\\r')\n",
    "    mean_accuracy = np.mean(accuracy_step)\n",
    "    accuracy_epoch.append(mean_accuracy)\n",
    "    loss_epoch.append(loss_step)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_val = []\n",
    "    valoader = ramloader_light(16, ground_truth_val,batch_last_val)\n",
    "    accuracy_step = []\n",
    "    loss_step = []\n",
    "    for i, data in enumerate(valoader):\n",
    "\n",
    "        layer_init = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "\n",
    "        outputs = mymodel(layer_init)\n",
    "        accuracy = get_accuracy(outputs, labels)\n",
    "        prob = torch.sigmoid(outputs)\n",
    "        pred = torch.squeeze((prob > 0.50).type(torch.FloatTensor))\n",
    "        prediction_val.append(pred.cpu())\n",
    "        accuracy_step.append(accuracy)\n",
    "    mean_accuracy = np.mean(accuracy_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(normalize_pred(prediction_val), ground_truth_val[:15712]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/Riloff_twitter/riloff_sarc_train_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [torch.tensor([tokenizer.encode(i, truncation=True, max_length=50)]) for i in test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "    \n",
    "#     for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "#         features = bertweet(input_ids[i]) \n",
    "#         sentence_emb_9 = zero_padding(features[2][9])\n",
    "#         sentence_emb_10 = zero_padding(features[2][10])\n",
    "#         sentence_emb_11 = zero_padding(features[2][11])\n",
    "#         sentence_emb_12 = zero_padding(features[2][12]) \n",
    "\n",
    "#         sub_layers_last = torch.stack((sentence_emb_9, sentence_emb_10, sentence_emb_11, sentence_emb_12), dim= 1)\n",
    "        \n",
    "#         torch.save(sub_layers_last.float().clone(), '../data/new_approach/test/sarcasm_word/features/{}_.pt'.format(i))\n",
    "#         torch.save(torch.tensor(test.labels.iloc[i], dtype = torch.float), '../data/new_approach/test/sarcasm_word/labels/{}_.pt'.format(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "batch_initial = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_middle = torch.zeros((len(input_ids),4,1,768))\n",
    "batch_last = torch.zeros((len(input_ids),4,1,768))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        \n",
    "        features = bertweet(input_ids[i]) #extract sentence embedding 1 x 768 for each document\n",
    "        \n",
    "        sentence_emb_1 = torch.mean(features[2][1], dim=1).view(1, -1) #layer 1 \n",
    "        sentence_emb_2 = torch.mean(features[2][2], dim=1).view(1, -1)\n",
    "        sentence_emb_3 = torch.mean(features[2][3], dim=1).view(1, -1)\n",
    "        sentence_emb_4 = torch.mean(features[2][4], dim=1).view(1, -1)\n",
    "        sentence_emb_5 = torch.mean(features[2][5], dim=1).view(1, -1)\n",
    "        sentence_emb_6 = torch.mean(features[2][6], dim=1).view(1, -1)\n",
    "        sentence_emb_7 = torch.mean(features[2][7], dim=1).view(1, -1)\n",
    "        sentence_emb_8 = torch.mean(features[2][8], dim=1).view(1, -1)\n",
    "        sentence_emb_9 = torch.mean(features[2][9], dim=1).view(1, -1)\n",
    "        sentence_emb_10 = torch.mean(features[2][10], dim=1).view(1, -1)\n",
    "        sentence_emb_11 = torch.mean(features[2][11], dim=1).view(1, -1)\n",
    "        sentence_emb_12 = torch.mean(features[2][12], dim=1).view(1, -1) #layer 12\n",
    "        # B x C x H x W, 1 x 4 x 1 x 768\n",
    "        sub_layers_initial = torch.stack((sentence_emb_1, sentence_emb_2, sentence_emb_3, sentence_emb_4), dim= 1).reshape(1,4,1,768)  #add batch dimension\n",
    "        sub_layers_middle = torch.stack((sentence_emb_5, sentence_emb_6, sentence_emb_7, sentence_emb_8), dim= 1).reshape(1,4,1,768)\n",
    "        sub_layers_last = torch.stack((sentence_emb_9, sentence_emb_10, sentence_emb_11, sentence_emb_12), dim= 1).reshape(1,4,1,768)\n",
    "              \n",
    "        batch_initial[i,:] = sub_layers_initial\n",
    "        batch_middle[i,:] = sub_layers_middle\n",
    "        batch_last[i,:] = sub_layers_last\n",
    "        \n",
    "        y_test.append(test.labels.iloc[i])\n",
    "\n",
    "ground_test = torch.tensor(y_test, dtype = torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_val = []\n",
    "    valoader = ramloader(16, ground_test,batch_initial, batch_middle, batch_last)\n",
    "    accuracy_step = []\n",
    "    loss_step = []\n",
    "    for i, data in enumerate(valoader):\n",
    "\n",
    "        layer_init = data[0].to(device)\n",
    "        layer_mid = data[1].to(device)\n",
    "        layer_last = data[2].to(device)\n",
    "        labels = data[3].to(device)\n",
    "\n",
    "        outputs = mymodel(layer_init, layer_mid, layer_last)\n",
    "        accuracy = get_accuracy(outputs, labels)\n",
    "        prob = torch.sigmoid(outputs)\n",
    "        pred = torch.squeeze((prob > 0.50).type(torch.FloatTensor))\n",
    "        prediction_val.append(pred.cpu())\n",
    "        accuracy_step.append(accuracy)\n",
    "    mean_accuracy = np.mean(accuracy_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less feautures test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    valoader = ramloader_light(2, ground_test,batch_last)\n",
    "    accuracy_step = []\n",
    "    loss_step = []\n",
    "    prediction_test = []\n",
    "    for i, data in enumerate(valoader):\n",
    "\n",
    "        layer_init = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "       \n",
    "        outputs = mymodel(layer_init)\n",
    "        accuracy = get_accuracy(outputs, labels)\n",
    "        \n",
    "        prob = torch.sigmoid(outputs)\n",
    "        pred = torch.squeeze((prob > 0.5).type(torch.FloatTensor))\n",
    "        prediction_test.append(pred.cpu())\n",
    "        accuracy_step.append(accuracy)\n",
    "    mean_accuracy = np.mean(accuracy_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(normalize_pred(prediction_val), test.labels[:1952]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(normalize_pred(prediction_val), test.labels[:1952], average='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
