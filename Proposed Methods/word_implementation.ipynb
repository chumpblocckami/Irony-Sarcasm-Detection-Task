{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer \n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from torch import nn\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "from Attention_Augmented_Conv2d.attention_augmented_conv import AugmentedConv\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simple_attention, self).__init__()\n",
    "        \n",
    "        self.conv_att = AugmentedConv(in_channels=4, out_channels=256, kernel_size=2, dk=3, dv=3, Nh=3, relative=False, stride=2)\n",
    "        self.pooling1 = nn.AvgPool3d(kernel_size=(1,1,1), stride = (2,1,1))\n",
    "        self.conv1 = nn.Conv2d(in_channels = 128, out_channels=64, kernel_size=2, stride = 2)\n",
    "        self.pooling2 =  nn.AvgPool3d(kernel_size=(1,1,1), stride = (2,1,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels=16, kernel_size=1, stride = 2)\n",
    "        self.max_pool = nn.MaxPool3d(kernel_size=(2,3,2), stride = (2,3,2))\n",
    "        self.bgru = nn.GRU(input_size=192, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc4 = nn.Linear(32,1)\n",
    "    \n",
    "    def forward(self, input1):\n",
    "        conv_atten = self.conv_att(input1)\n",
    "        conv_pooled1 = self.pooling1(conv_atten)\n",
    "        conv_simple = self.conv1(conv_pooled1)\n",
    "        conv_pooled2 = self.pooling2(conv_simple)\n",
    "        conv_simple = self.conv2(conv_pooled2)\n",
    "        conv_max = self.max_pool(conv_simple)\n",
    "        flatten = torch.flatten(conv_max).reshape(conv_max.size(0), 1, 192)\n",
    "        output_gru, hidden_gru = self.bgru(flatten)\n",
    "        dense1 = F.relu(self.fc1(output_gru))\n",
    "        dense2 = F.relu(self.fc2(dense1))\n",
    "        dense2_drop = self.dropout(dense2)\n",
    "        dense3 = F.relu(self.fc3(dense2_drop))\n",
    "        dense3_drop = self.dropout(dense3)\n",
    "        output = self.fc4(dense3_drop)\n",
    "    \n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = simple_attention()\n",
    "mymodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.abspath('.')) \n",
    "data_dir = ROOT_DIR + '\\\\{}\\\\{}\\\\{}\\\\{}\\\\'.format('data', 'new_approach', 'train', 'sarcasm_word')\n",
    "features_dir = data_dir + 'features\\\\'\n",
    "label_dir = data_dir + 'labels\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_number(elem):\n",
    "    return int(re.findall('(\\d*)(_.pt)', elem)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_feat, root_label):\n",
    "        self.files = os.listdir(root_feat)\n",
    "        self.labels = os.listdir(root_label)\n",
    "        self.files.sort(reverse=False, key=sort_number)\n",
    "        self.labels.sort(reverse=False, key=sort_number)\n",
    "        \n",
    "        self.root_feat = root_feat\n",
    "        self.root_label = root_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.load(os.path.join(self.root_feat, self.files[idx])) # load the features of this sample\n",
    "        label = torch.load(os.path.join(self.root_label, self.labels[idx]))\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def get_accuracy(output, actual):\n",
    "    \"\"\"\n",
    "    Return the accuracy of the model on the input data and actual ground truth.\n",
    "    \"\"\"\n",
    "    prob = torch.sigmoid(output)\n",
    "   \n",
    "    pred = torch.squeeze((prob > 0.50).type(torch.FloatTensor), -1)\n",
    "    \n",
    "    accuracy = accuracy_score(torch.squeeze(pred,0).cpu(), actual.cpu())\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pred(pred):\n",
    "    numpy_list = [i.numpy() for i in pred]\n",
    "    numpy_1vec = np.concatenate(numpy_list).ravel()\n",
    "    return numpy_1vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(features_dir,label_dir)\n",
    "trainloader = torch.utils.data.DataLoader(dataset,shuffle=True,batch_size=5,num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = ROOT_DIR + '\\\\{}\\\\{}\\\\{}\\\\{}\\\\'.format('data', 'new_approach', 'validation', 'sarcasm_word')\n",
    "features_dir = data_dir + 'features\\\\'\n",
    "label_dir = data_dir + 'labels\\\\'\n",
    "dataset_val = MyDataset(features_dir,label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_lr(stepsize, min_lr=2e-5, max_lr=1e-3):\n",
    "\n",
    "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
    "    scaler = lambda x: 1.\n",
    "\n",
    "    # Lambda function to calculate the LR\n",
    "    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
    "\n",
    "    # Additional function to see where on the cycle we are\n",
    "    def relative(it, stepsize):\n",
    "        cycle = math.floor(1 + it / (2 * stepsize))\n",
    "        x = abs(it / stepsize - 2 * cycle + 1)\n",
    "        return max(0, (1 - x)) * scaler(cycle)\n",
    "\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(mymodel.parameters(), lr=0.1, momentum = 0.9)\n",
    "total_epochs = 10\n",
    "batch_size = 5\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, verbose=True, min_lr = 0.0001)\n",
    "optimizer = optim.SGD(mymodel.parameters(), lr=0.001, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.1,verbose=True)\n",
    "# optimizer = torch.optim.Adam(mymodel.parameters(), lr=1.)\n",
    "# step_size = len(data_iter_train)\n",
    "clr = cyclical_lr(5000,  min_lr=2e-5, max_lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])\n",
    "#torch.optim.lr_scheduler.CyclicLR( optimizer , base_lr = 0.00001 , max_lr = 0.001, step_size_up = 2000 , step_size_down = None , mode = 'triangular' , gamma = 1.0 , scale_fn = None , scale_mode = 'cycle' , cycle_momentum = True , base_momentum = 0.8 , max_momentum = 0.9 , last_epoch = - 1 , verbose = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.load_state_dict(torch.load(\"../Code/model_pytorch/model_0.711.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_epoch = []\n",
    "loss_epoch = []\n",
    "accuracy_val_epoch = []\n",
    "loss_val_epoch = []\n",
    "best_val = 0\n",
    "for step in range(total_epochs):\n",
    "    trainloader = torch.utils.data.DataLoader(dataset,shuffle=True,batch_size=batch_size,num_workers=0, pin_memory=True)\n",
    "    accuracy_step = []\n",
    "    loss_step = []\n",
    "    accuracy_step = []\n",
    "    loss_step = []\n",
    "    for i, data in enumerate(trainloader):\n",
    "        input_embeddings = data[0].to(device)\n",
    "\n",
    "        labels = torch.unsqueeze(data[1].to(device), -1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mymodel(torch.squeeze(input_embeddings, 1))\n",
    "        loss = criterion(outputs, torch.unsqueeze(labels, -1))\n",
    "        torch.cuda.empty_cache()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step() \n",
    "        \n",
    "        accuracy = get_accuracy(outputs, labels)\n",
    "        accuracy_step.append(accuracy)\n",
    "        loss_step.append(loss)    \n",
    "\n",
    "        print('Epoch {}, Step: {} / {}, Loss: {}, Accuracy: {}'.format(step, i, len(trainloader), loss, accuracy), end = '\\r')\n",
    "        scheduler.step()\n",
    "    mean_accuracy = np.mean(accuracy_step)\n",
    "    accuracy_epoch.append(mean_accuracy)\n",
    "    loss_epoch.append(torch.mean(torch.tensor(loss_step)))\n",
    "    print(\"Accuracy epoch {}: {}\".format(step, mean_accuracy), end = '\\r')\n",
    "    with torch.no_grad():\n",
    "        valoader = torch.utils.data.DataLoader(dataset_val,shuffle=True,batch_size=batch_size,num_workers=0, pin_memory=True)\n",
    "        accuracy_step = []\n",
    "        loss_step = []\n",
    "        for i, data in enumerate(valoader):\n",
    "\n",
    "            input_embeddings = data[0].to(device)\n",
    "            labels = torch.unsqueeze(data[1].to(device), -1)\n",
    "            \n",
    "            outputs = mymodel(torch.squeeze(input_embeddings, 1))\n",
    "            \n",
    "            loss_val = criterion(outputs, torch.unsqueeze(labels, -1))\n",
    "            accuracy = get_accuracy(outputs, labels)\n",
    "            accuracy_step.append(accuracy)\n",
    "            loss_step.append(loss_val)\n",
    "            \n",
    "        mean_accuracy = np.mean(accuracy_step)\n",
    "        accuracy_val_epoch.append(mean_accuracy)\n",
    "        loss_val_epoch.append(torch.mean(torch.tensor(loss_step)))\n",
    "        \n",
    "        if mean_accuracy > best_val:\n",
    "            best_val = mean_accuracy\n",
    "            torch.save(mymodel.state_dict(), '../Code/model_pytorch/model_{}.pt'.format(best_val.round(3)))\n",
    "    #scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
